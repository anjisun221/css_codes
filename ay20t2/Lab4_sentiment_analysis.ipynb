{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pnxbJzbv_Jsr"
   },
   "source": [
    "# Lab 4 - Sentiment analysis from texts\n",
    "\n",
    "In this lab, you will learn:\n",
    "* How to clean texts\n",
    "* How to generate Document-Term matrix from texts\n",
    "* How to do sentiment analysis from texts\n",
    "\n",
    "This lab is written by Jisun AN (jisunan@smu.edu.sg) and Michelle KAN (michellekan@smu.edu.sg).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VBe-9KkA_Jsw"
   },
   "source": [
    "# 1. Getting the data\n",
    "\n",
    "In this lab, we will use restaurant review data. \n",
    "\n",
    "This data is manually annotated by humans according to their aspect and sentiment. \n",
    "\n",
    "One review may have two or more aspects and thus two ore more sentiment. \n",
    "\n",
    "We note that we excluded those conflicting reviews.\n",
    "\n",
    "\"restaurant_reviews.tsv\" is tab-separated file which fields are: \n",
    "\n",
    "- `sid` is review id\n",
    "- `text` is a review\n",
    "- `aspect` refers to the review area of interest. It consists of any of these five labels: <i>food, service, ambience, price</i> \n",
    "- `sentiment` consists of one of these labels: <i>positive, negative, neutral</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2136,
     "status": "ok",
     "timestamp": 1612159073697,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "7Yig1Syu_Jsx"
   },
   "outputs": [],
   "source": [
    "# Import Pandas to analyze the data\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "executionInfo": {
     "elapsed": 2528,
     "status": "ok",
     "timestamp": 1612159077839,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "nEqga9U6_Jsx",
    "outputId": "79c301c5-1c23-41f3-ca7c-4cba5809ddaf"
   },
   "outputs": [],
   "source": [
    "# Read the file using Pandas 'read_table' function (either read_table, read_csv is fine)\n",
    "ori_df = pd.read_table(\"https://raw.githubusercontent.com/anjisun221/css_codes/main/restaurant_reviews.tsv\", sep=\"\\t\")\n",
    "\n",
    "print(ori_df.shape)\n",
    "ori_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 2432,
     "status": "ok",
     "timestamp": 1612159084126,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "qCAV4Vqy_Jsy",
    "outputId": "c7d6643b-4ba9-4a1d-e806-ba86870abe68"
   },
   "outputs": [],
   "source": [
    "# to see entire text \n",
    "pd.set_option('display.max_colwidth', 150)\n",
    "ori_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1711,
     "status": "ok",
     "timestamp": 1612159088483,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "ZuBAg7Cj_Jsy",
    "outputId": "9e01f27f-7d25-4f4c-96e8-2b2a0662db6f"
   },
   "outputs": [],
   "source": [
    "ori_df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1657,
     "status": "ok",
     "timestamp": 1612159091266,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "haRWORA2_Jsy",
    "outputId": "69abd7d6-791f-45b8-c5b1-cf2e3e2a7bbe"
   },
   "outputs": [],
   "source": [
    "ori_df['aspect'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDlS1tMF_Jsz"
   },
   "source": [
    "### Combine review by aspect + sentiment (e.g., all positive reviews about food)\n",
    "\n",
    "The following code creates a new dictionary `data_combined` with the following <i>key-value</i> pairs:\n",
    "- key: a concatenated string of `aspect_sentiment` e.g, for a positive review which is related to food, the label will be <i>food_positive</i>\n",
    "- value: the text of the review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1501,
     "status": "ok",
     "timestamp": 1612159094961,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "qsGKDAQt_Jsz",
    "outputId": "ba7a91d8-cd06-4679-d9b3-978764ae01f8"
   },
   "outputs": [],
   "source": [
    "list_sentiment = ['positive', 'negative', 'neutral']\n",
    "list_aspect = ['food', 'service', 'ambience', 'price']\n",
    "\n",
    "data_combined = {}\n",
    "\n",
    "for each_sent in list_sentiment:\n",
    "    for each_aspect in list_aspect:\n",
    "        \n",
    "        # concatenate aspect and sentiment, and assign to new_label\n",
    "        new_label = each_aspect+\"_\"+each_sent\n",
    "        print(new_label)\n",
    "        \n",
    "        # query dataframe to extract text associated with the aspect and sentiment\n",
    "        tmp_df = ori_df.query(\"sentiment==@each_sent and aspect==@each_aspect\")\n",
    "        texts = \" \".join(tmp_df['text'].to_list())\n",
    "        data_combined[new_label] = [texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2053,
     "status": "ok",
     "timestamp": 1612159098553,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "JuFzGRt5_Jsz",
    "outputId": "95455057-381f-4ebd-e2d1-06e72ac865ca"
   },
   "outputs": [],
   "source": [
    "data_combined.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5S0QVEap_Jsz"
   },
   "source": [
    "Turn dictionary into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 1951,
     "status": "ok",
     "timestamp": 1612159101383,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "As2_tu9-_Jsz",
    "outputId": "ab5f4fd7-e84d-439b-8d8e-3ea0f544a0c7"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(data_combined, orient='index')\n",
    "df.columns = ['text']\n",
    "df = df.sort_index()\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137
    },
    "executionInfo": {
     "elapsed": 738,
     "status": "ok",
     "timestamp": 1612159362629,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "x44xtpEH_Js0",
    "outputId": "96a9f6e0-2270-4320-e29b-a4977daa7531",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's take a look at the negative reviews for ambience\n",
    "df.text.loc['ambience_negative']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_DcUkYyM_Js0"
   },
   "source": [
    "Let's save this dataframe to the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "79d-nz2a_Js0"
   },
   "source": [
    "# 2. Cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "orQH1Z1P_Js0"
   },
   "source": [
    "When dealing with numerical data, data cleaning often involves removing null values and duplicate data, dealing with outliers, etc. With text data, there are some common data cleaning techniques, which are also known as text pre-processing techniques.\n",
    "\n",
    "With text data, this cleaning process can go on forever. There's always an exception to every cleaning step. So, we're going to follow the MVP (minimum viable product) approach - start simple and iterate. Here are a bunch of things you can do to clean your data. We're going to execute just the common cleaning steps here and the rest can be done at a later point to improve our results.\n",
    "\n",
    "**Common data cleaning steps on all text:**\n",
    "* Make text all lower case\n",
    "* Remove punctuation\n",
    "* Remove numerical values\n",
    "* Remove common non-sensical text (/n)\n",
    "* Tokenize text\n",
    "* Remove stop words\n",
    "\n",
    "**More data cleaning steps after tokenization:**\n",
    "* Stemming / lemmatization\n",
    "* Parts of speech tagging\n",
    "* Create bi-grams or tri-grams\n",
    "* Deal with typos\n",
    "* And more..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "egoiXlSq_Js0"
   },
   "source": [
    "## Round 1. Let's convert text to lowercase, remove punctuations, remove words containing numbers.\n",
    "\n",
    "Python has a built-in package called `re`, which can be used to work with Regular Expressions. Regular Expression, is a sequence of characters that forms a search pattern.<br>\n",
    "\n",
    "The `re.sub()` function can be used to replace substrings. The syntax `re.sub(pattern,repl,text)` replaces the pattern matches in text with repl. In the following code, it is used to remove punctuations and remove words containing number. You can read up about `re.sub()` [here](https://www.w3schools.com/python/python_regex.asp) and [here](https://www.pythonforbeginners.com/regex/regular-expressions-in-python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 803,
     "status": "ok",
     "timestamp": 1612159370922,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "JSKxJUWu_Js1"
   },
   "outputs": [],
   "source": [
    "# Apply a first round of text cleaning techniques\n",
    "import re \n",
    "import string\n",
    "\n",
    "def clean_text_round1(text):\n",
    "    '''Make text lowercase, remove punctuation and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 896,
     "status": "ok",
     "timestamp": 1612159378206,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "SF4n0-7Z_Js1",
    "outputId": "0a72538c-4ed8-4a85-8462-404b424c29e6"
   },
   "outputs": [],
   "source": [
    "# Let's take a look at the updated text\n",
    "df_clean = pd.DataFrame(df['text'].apply(clean_text_round1))\n",
    "df_clean.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dn6CSU54JzSG"
   },
   "source": [
    "## Additional exercise 1 (optional)\n",
    "\n",
    "1. Get more familar with regular expression below. \n",
    "\n",
    "2. Can you remove url from a tweet using regular expression? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OqzpLSMhJzSG"
   },
   "outputs": [],
   "source": [
    "# Examples of Regular expression \n",
    "import re # Need module 're' for regular expression\n",
    "\n",
    "# Try find: re.findall(regexStr, inStr) -> matchedSubstringsList\n",
    "# r'...' denotes raw strings which ignore escape code, i.e., r'\\n' is '\\'+'n'\n",
    "# [0-9] matches any digit; [A-Za-z] matches any uppercase or lowercase letters.\n",
    "# + means one or more\n",
    "print(re.findall(r'[0-9]+', 'abc123xyz')) # Return a list of matched substrings.  \n",
    "print(re.findall(r'[0-9]+', 'abcxyz')) # Return []\n",
    "print(re.findall(r'[0-9]+', 'abc00123xyz456_0')) # Return ['00123', '456', '0']\n",
    "\n",
    "\n",
    "# Try substitute: re.sub(regexStr, replacementStr, inStr) -> outStr\n",
    "# Below code will replace all number block to *\n",
    "print(re.sub(r'[0-9]+', r'*', 'abc00123xyz456_0')) # Return 'abc*xyz*_*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OZLVtTHcJzSG"
   },
   "outputs": [],
   "source": [
    "# Here's an example tweet\n",
    "mytweet = \"New pre-print that @GruppiMauricio, @sibel_adali and I have been holding on to for a while: https://arxiv.org/abs/2101.10973. The goal was to leverage content sharing practices by news outlets in news veracity detection. Thread.\"\n",
    "mytweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3vVAY8PqJzSH"
   },
   "source": [
    "Write the code to remove URL from the above tweet.\n",
    "\n",
    "As a result, you should see \n",
    "\n",
    "'New pre-print that @GruppiMauricio, @sibel_adali and I have been holding on to for a while:  The goal was to leverage content sharing practices by news outlets in news veracity detection. Thread.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xgJs6J-sJzSH"
   },
   "outputs": [],
   "source": [
    "text = # Write your code \n",
    "text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BPvxSGDd_Js1"
   },
   "source": [
    "## Round 2. Let's remove stopwords. \n",
    "\n",
    "A stop word is a commonly used word (such as \"the\", \"a\", \"an\", \"in\"). For some analysis, like looking into top words, those stop words are often meaningless, and thus we remove them.\n",
    "\n",
    "The [Natural Language Toolkit (nltk)](https://www.nltk.org/api/nltk.html) is a Python package for natural language processing. We will import the library for the removal of stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2852,
     "status": "ok",
     "timestamp": 1612159387226,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "0IG3CWjb_Js2",
    "outputId": "05480210-ef61-4d7c-c55c-00f62e3b2095"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UmiMDOChJzSI"
   },
   "source": [
    "The `nltk` library has a list of stopwords stored in 16 different languages. We will retrieve the list of English stop words using `stopwords.words('english')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 969,
     "status": "ok",
     "timestamp": 1612159389877,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "UhKTgIXoJzSI"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# Exclude stopwords with Python's list comprehension and pandas.DataFrame.apply.\n",
    "def clean_text_round2(text):\n",
    "    return ' '.join([word for word in text.split() if word not in (stop)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JWNTnZ2rJzSI"
   },
   "source": [
    "The expression `[word for word in x.split() if word not in (stop)]` is a list comprehension.<br>\n",
    "List comprehension offers a shorter syntax when you want to create a new list based on the values of an existing list.<br> Syntax: `new_list = [expression for item in iterable list if condition == True]`<br> \n",
    "You can read up about list comprehension [here](https://www.w3schools.com/python/python_lists_comprehension.asp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 830,
     "status": "ok",
     "timestamp": 1612159393241,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "gPXZ2o6k_Js2",
    "outputId": "80caadaf-a88b-441b-bccb-8da9882dc458"
   },
   "outputs": [],
   "source": [
    "df_clean = pd.DataFrame(df_clean['text'].apply(clean_text_round2))\n",
    "df_clean.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p4HjENg8_Js2"
   },
   "source": [
    "**NOTE:** This data cleaning aka text pre-processing step could go on for a while, but we are going to stop for now. After going through some analysis techniques, if you see that the results don't make sense or could be improved, you can come back and make more edits such as:\n",
    "* Mark 'outstanding' and 'outstand' as the same word (stemming / lemmatization)\n",
    "* Combine 'thank you' into one term (bi-grams)\n",
    "* And a lot more..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dPbFZzLi_Js2"
   },
   "source": [
    "## Organizing the data\n",
    "\n",
    "We will have clean, organized data in two standard text formats:\n",
    "\n",
    "1. **Corpus - **a collection of text\n",
    "2. **Document-Term Matrix - **word counts in matrix format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8gQsj1Ca_Js3"
   },
   "source": [
    "### Corpus\n",
    "\n",
    "We already created a corpus in an earlier step. The definition of a corpus is a collection of texts, and they are all put together neatly in a pandas dataframe here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "executionInfo": {
     "elapsed": 797,
     "status": "ok",
     "timestamp": 1612159419453,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "tIjzk5Yb_Js3",
    "outputId": "72a7ad1d-be29-4877-e197-a564b28e944b"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "executionInfo": {
     "elapsed": 996,
     "status": "ok",
     "timestamp": 1612159422656,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "mDGV8HOh_Js3",
    "outputId": "993ff7c3-bf25-4ba1-d892-94a58f004ffc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's add the full label as well\n",
    "full_labels = ['ambience_negative', 'ambience_neutral', 'ambience_positive',\n",
    "       'food_negative', 'food_neutral', 'food_positive', 'price_negative',\n",
    "       'price_neutral', 'price_positive', 'service_negative',\n",
    "       'service_neutral', 'service_positive']\n",
    "\n",
    "df['category'] = full_labels\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 881,
     "status": "ok",
     "timestamp": 1612159425116,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "ZsRt-4Q0_Js3"
   },
   "outputs": [],
   "source": [
    "# Let's pickle it for later use\n",
    "# Pickling allows you to save a python object as a .pkl binary file on your hard drive.\n",
    "df.to_pickle(\"corpus.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eH0CjMNZ_Js3"
   },
   "source": [
    "### Document-Term Matrix\n",
    "\n",
    "For many of the techniques we'll be using in future notebooks, the text must be tokenized, meaning broken down into smaller pieces. The most common tokenization technique is to break down text into words. We can do this using scikit-learn's [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html), where every row will represent a different document and every column will represent a different word.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    },
    "executionInfo": {
     "elapsed": 905,
     "status": "ok",
     "timestamp": 1612159428289,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "rX-V1RKW_Js4",
    "outputId": "661c9ae5-7a48-4e93-ebc4-b3d4e1909065"
   },
   "outputs": [],
   "source": [
    "# We are going to create a document-term matrix using CountVectorizer, and exclude common English stop words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(stop_words='english') #You can remove stop words using CountVectorizer as well\n",
    "data_cv = cv.fit_transform(df_clean.text)\n",
    "data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "data_dtm.index = df.index\n",
    "data_dtm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 884,
     "status": "ok",
     "timestamp": 1612159430856,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "zk7qmIhF_Js4"
   },
   "outputs": [],
   "source": [
    "# Let's pickle it for later use\n",
    "data_dtm.to_pickle(\"dtm.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 777,
     "status": "ok",
     "timestamp": 1612159432593,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "vJ2b7u-Q_Js4"
   },
   "outputs": [],
   "source": [
    "# Let's also pickle the cleaned data (before we put it in document-term matrix format) and the CountVectorizer object\n",
    "import pickle\n",
    "df_clean.to_pickle('data_clean.pkl')\n",
    "pickle.dump(cv, open(\"cv.pkl\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FbDt9BM2_Js4"
   },
   "source": [
    "## Additional exercise 2 (optional)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8bz3QE19_Js4"
   },
   "source": [
    "1. Can you add an additional regular expression to the clean_text_round1 function to further clean the text?\n",
    "2. Play around with CountVectorizer's parameters. What is ngram_range? What is min_df and max_df?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mAabM6_y_Js4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azMBzU-o_Js5"
   },
   "source": [
    "# 3. Exploratory Data Analysis\n",
    "\n",
    "After the data cleaning step where we put our data into a few standard formats, the next step is to take a look at the data and see if what we're looking at makes sense. Before applying any fancy algorithms, it's always important to explore the data first.\n",
    "\n",
    "We are going to look at the **Most common words** and **Amount of love/hate** for each category.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gSH1rQaG_Js5"
   },
   "source": [
    "## Most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "executionInfo": {
     "elapsed": 820,
     "status": "ok",
     "timestamp": 1612159437508,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "IL8MZfWt_Js5",
    "outputId": "74651bbc-5f47-48ab-c98d-00cfd0344df3"
   },
   "outputs": [],
   "source": [
    "# If you start from Section 3, please uncomment below code\n",
    "# import pandas as pd\n",
    "# data_dtm = pd.read_pickle('./dtm.pkl')\n",
    "\n",
    "data = data_dtm.transpose() #transpose changes columns to rows and rows to columns\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 784,
     "status": "ok",
     "timestamp": 1612159441085,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "wodQnuj7_Js5",
    "outputId": "92cdd270-3aa3-4b93-dbb9-42a956f052cf"
   },
   "outputs": [],
   "source": [
    "# Find the top 30 words said by each category\n",
    "top_dict = {}\n",
    "for c in data.columns:\n",
    "    top = data[c].sort_values(ascending=False).head(30)\n",
    "    top_dict[c]= list(zip(top.index, top.values))\n",
    "\n",
    "top_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 821,
     "status": "ok",
     "timestamp": 1612159446641,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "h-dMilJt_Js5",
    "outputId": "847babed-ae3e-4bf3-b06f-a9cf7bb31558",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print the top 15 words said by each category\n",
    "for category, top_words in top_dict.items():\n",
    "    print(category)\n",
    "    print(', '.join([word for word, count in top_words[0:14]]))\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8BoMXQr_Js6"
   },
   "source": [
    "**NOTE:** At this point, we could go on and create word clouds. However, by looking at these top words, you can see that some of them have very little meaning and could be added to a stop words list, so let's do just that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 817,
     "status": "ok",
     "timestamp": 1612159449983,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "WDge8zuF_Js6",
    "outputId": "45d24854-8d64-455b-8049-c1abf007420b"
   },
   "outputs": [],
   "source": [
    "# Look at the most common top words --> add them to the stop word list\n",
    "# Counter makes it easy to count values in a list see more on https://docs.python.org/3/library/collections.html#collections.Counter\n",
    "from collections import Counter\n",
    "\n",
    "# Let's first pull out the top 30 words for each category\n",
    "words = []\n",
    "for category in data.columns:\n",
    "    top = [word for (word, count) in top_dict[category]]\n",
    "    for t in top:\n",
    "        words.append(t)\n",
    "        \n",
    "words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 808,
     "status": "ok",
     "timestamp": 1612159478557,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "PYNz97Te_Js6",
    "outputId": "41b7f55f-abb1-4952-cffd-9dd35fecb28f"
   },
   "outputs": [],
   "source": [
    "# Let's aggregate this list and identify the most common words along with how many categories they occur in\n",
    "Counter(words).most_common()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 848,
     "status": "ok",
     "timestamp": 1612159571224,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "ipjL7OIW_Js6",
    "outputId": "4dc7c36c-8294-454f-b013-33d3cc16283a"
   },
   "outputs": [],
   "source": [
    "# If more than half of the categories (6) have it as a top word, exclude it from the list \n",
    "add_stop_words = [word for word, count in Counter(words).most_common() if count > 6]\n",
    "add_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 870,
     "status": "ok",
     "timestamp": 1612159573552,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "FONfTCS8_Js6"
   },
   "outputs": [],
   "source": [
    "# Let's update our document-term matrix with the new list of stop words\n",
    "from sklearn.feature_extraction import text \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Read in cleaned data\n",
    "df_clean = pd.read_pickle('./data_clean.pkl')\n",
    "\n",
    "# Add new stop words\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n",
    "\n",
    "# Recreate document-term matrix\n",
    "cv = CountVectorizer(stop_words=stop_words)\n",
    "data_cv = cv.fit_transform(df_clean.text)\n",
    "data_stop = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "data_stop.index = df_clean.index\n",
    "\n",
    "# # Pickle it for later use\n",
    "# If you start from Section 3, please uncomment below code\n",
    "# import pickle\n",
    "pickle.dump(cv, open(\"cv_stop.pkl\", \"wb\"))\n",
    "data_stop.to_pickle(\"dtm_stop.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 740,
     "status": "ok",
     "timestamp": 1612159577593,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "tu3PC97e_Js7"
   },
   "outputs": [],
   "source": [
    "# Let's make some word clouds!\n",
    "# Terminal / Anaconda Prompt: conda install -c conda-forge wordcloud\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "wc = WordCloud(stopwords=stop_words, background_color=\"white\", colormap=\"Dark2\",\n",
    "               max_font_size=150, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "executionInfo": {
     "elapsed": 6339,
     "status": "ok",
     "timestamp": 1612159585812,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "SXPXVyRu_Js7",
    "outputId": "8d01aa0b-a39e-49aa-85ff-0d0afc8d7d72"
   },
   "outputs": [],
   "source": [
    "# Reset the output dimensions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [16, 6]\n",
    "\n",
    "full_labels = ['ambience_negative', 'ambience_neutral', 'ambience_positive',\n",
    "       'food_negative', 'food_neutral', 'food_positive', 'price_negative',\n",
    "       'price_neutral', 'price_positive', 'service_negative',\n",
    "       'service_neutral', 'service_positive']\n",
    "\n",
    "# Create subplots for each category\n",
    "for index, category in enumerate(data.columns):\n",
    "    wc.generate(df_clean.text[category])\n",
    "    \n",
    "    plt.subplot(3, 4, index+1)\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(full_labels[index])\n",
    "    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G7BtvXWT_Js7"
   },
   "source": [
    "Findings \n",
    "\n",
    "* Reviews of different aspects seems to have different set of frequent words. E.g., ambience : table, atmosphere, decor, etc whiel price : price, worth, cheap, etc.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WSdQYoda_Js8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Tk34a_F_Js8"
   },
   "source": [
    "## Amount of love/hate words in positive/negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "executionInfo": {
     "elapsed": 821,
     "status": "ok",
     "timestamp": 1612159611064,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "1_PPsyKI_Js8",
    "outputId": "bc15eeb0-3632-4068-e9e3-dd1bfa2f97d2"
   },
   "outputs": [],
   "source": [
    "# Let's isolate just these words\n",
    "data_lovehate_words = data.transpose()[['love', 'best', 'dont', 'worst', 'didnt']]\n",
    "data_lovehate = pd.concat([data_lovehate_words.love + data_lovehate_words.best, data_lovehate_words.dont + data_lovehate_words.worst + data_lovehate_words.didnt], axis=1)\n",
    "data_lovehate.columns = ['love', 'hate']\n",
    "data_lovehate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 414
    },
    "executionInfo": {
     "elapsed": 1537,
     "status": "ok",
     "timestamp": 1612159618195,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "9igvKHXy_Js9",
    "outputId": "f154fa0c-a85b-4760-881d-4a0fe4e92a51"
   },
   "outputs": [],
   "source": [
    "# Let's create a scatter plot of our findings\n",
    "plt.rcParams['figure.figsize'] = [12, 6]\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for i, category in enumerate(data_lovehate.index):\n",
    "    x = data_lovehate.love.loc[category]\n",
    "    y = data_lovehate.hate.loc[category]\n",
    "    plt.scatter(x, y, color='blue')\n",
    "    plt.text(x+1.5, y+0.5, full_labels[i], fontsize=10)\n",
    "    plt.xlim(-5, 85) \n",
    "\n",
    "plt.title('Number of Love/Hate Words Used in Reviews', fontsize=20)\n",
    "plt.xlabel('Number of love/best', fontsize=15)\n",
    "plt.ylabel('Number of dont/didnt/worst', fontsize=15)\n",
    "\n",
    "xpoints = ypoints = plt.xlim()\n",
    "plt.plot(xpoints, ypoints, linestyle='--', color='k', lw=3, scalex=False, scaley=False)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_gXhck-_Js9"
   },
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "huKrLjIK_Js9"
   },
   "source": [
    "What other word counts do you think would be interesting to compare instead of the love/hate words? Create a scatter plot comparing them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1mHAbHDc_Js-"
   },
   "outputs": [],
   "source": [
    "# Write your code here \n",
    "# If you get an error that you don't have 'data' defined, please uncomment below\n",
    "# import pandas as pd\n",
    "# data_dtm = pd.read_pickle('./dtm.pkl')\n",
    "# data = data_dtm.transpose()\n",
    "\n",
    "\n",
    "# Let's isolate some words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lFZzPHZI_Js-"
   },
   "outputs": [],
   "source": [
    "# Let's create a scatter plot of your findings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tCIdpkE6_Js-"
   },
   "source": [
    "# 4. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-CMw56mj_Js-"
   },
   "source": [
    "We will examine whether sentiment analysis method is useful to distinguish positive/neutral/negative reviews. \n",
    "\n",
    "In this lab, we will use **TextBlob** for sentiment analysis.\n",
    "\n",
    "1. **TextBlob Module:** Linguistic researchers have labeled the sentiment of words based on their domain expertise. Sentiment of words can vary based on where it is in a sentence. The TextBlob module allows us to take advantage of these labels.\n",
    "2. **Sentiment Labels:** Each word in a corpus is labeled in terms of polarity and subjectivity (there are more labels as well, but we're going to ignore them for now). A corpus' sentiment is the average of these.\n",
    "   * **Polarity**: How positive or negative a word is. -1 is very negative. +1 is very positive.\n",
    "   * **Subjectivity**: How subjective, or opinionated a word is. 0 is fact. +1 is very much an opinion.\n",
    "\n",
    "For more info on how TextBlob coded up its [sentiment function](https://planspace.org/20150607-textblob_sentiment/).\n",
    "\n",
    "Let's take a look at the sentiment of the various categories. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "executionInfo": {
     "elapsed": 1188,
     "status": "ok",
     "timestamp": 1612159653249,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "zlTDCtl0_Js-",
    "outputId": "9de35d57-dc7e-4091-f0b9-264ab1a752e0"
   },
   "outputs": [],
   "source": [
    "# If you start from Section 4, please uncomment below code\n",
    "# import pandas as pd\n",
    "\n",
    "# We'll start by reading in the corpus, which preserves word order\n",
    "data = pd.read_pickle('./corpus.pkl')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ROrerxbw_Js-"
   },
   "outputs": [],
   "source": [
    "# install textblob\n",
    "# !pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 707,
     "status": "ok",
     "timestamp": 1612159656066,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "DtC5UIVHJzSN"
   },
   "outputs": [],
   "source": [
    "# Create quick functions to find the polarity and subjectivity of each category using TextBlob\n",
    "\n",
    "def blob_polarity(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "def blob_subjectivity(text):\n",
    "    return TextBlob(text).sentiment.subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 625
    },
    "executionInfo": {
     "elapsed": 1457,
     "status": "ok",
     "timestamp": 1612159657445,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "7S0-z7jg_Js_",
    "outputId": "76f6452e-f0db-4e33-d4d0-75c92d4ac180"
   },
   "outputs": [],
   "source": [
    "# Terminal / Anaconda Navigator: conda install -c conda-forge textblob\n",
    "from textblob import TextBlob\n",
    "\n",
    "data['polarity'] = data['text'].apply(blob_polarity)\n",
    "data['subjectivity'] = data['text'].apply(blob_subjectivity)\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "executionInfo": {
     "elapsed": 1071,
     "status": "ok",
     "timestamp": 1612159664511,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "J0hCzEoq_Js_",
    "outputId": "32bee9c1-b8d1-44c7-d288-98a31b842381"
   },
   "outputs": [],
   "source": [
    "# Let's plot the results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 8]\n",
    "\n",
    "for index, category in enumerate(data.index):\n",
    "    x = data.polarity.loc[category]\n",
    "    y = data.subjectivity.loc[category]\n",
    "    \n",
    "    if 'positive' in category:\n",
    "        plt.scatter(x, y, color='blue')\n",
    "    elif 'negative' in category:\n",
    "        plt.scatter(x, y, color='red', marker='*')\n",
    "    else:\n",
    "        plt.scatter(x, y, color='green', marker='s')\n",
    "        \n",
    "    plt.text(x+.001, y+.001, data['category'][index], fontsize=10)\n",
    "    plt.xlim(-.1, .55) \n",
    "    \n",
    "plt.title('Sentiment Analysis', fontsize=20)\n",
    "plt.xlabel('<-- Negative -------- Positive -->', fontsize=15)\n",
    "plt.ylabel('<-- Facts -------- Opinions -->', fontsize=15)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_u8jKW_d_Js_"
   },
   "source": [
    "### Exercise 2\n",
    "\n",
    "Let's change the marker shapes. Change the shape of markers for negative reviews to a triangle.\n",
    "\n",
    "Matplotlib supports various shapes for markers. Please check the [official document](https://matplotlib.org/3.3.3/api/markers_api.html) for other markers. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gJ5DbT_BJzSO"
   },
   "outputs": [],
   "source": [
    "# Write your code \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0pG8eNJi_Js_"
   },
   "source": [
    "### Exercise 3\n",
    "\n",
    "Let's compare the sentiments of the reviews computed by Textblob and Vader\n",
    "\n",
    "You will need to apply Vader to analyze sentiment of reviews. \n",
    "\n",
    "1. Define the function that return compound score given a sentence\n",
    "2. Apply function 1 to compute vader score, the column name would be 'vader_sent' \n",
    "3. Draw a scatter plot to compare Vader sentiment score (x-axis) with TextBlob polarity score (y-axis)\n",
    "4. What's your conclusion?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5112,
     "status": "ok",
     "timestamp": 1612159677003,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "O1Adx8Cu_Js_",
    "outputId": "2165c43f-9f1b-4d3c-8982-f6c2d1dfaa80"
   },
   "outputs": [],
   "source": [
    "!pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4358,
     "status": "ok",
     "timestamp": 1612159677004,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "NGD8Vg3w_Js_"
   },
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j1ZP8cj9_Js_"
   },
   "outputs": [],
   "source": [
    "# 1. Write a python function that returns VADER's 'compound score' of a sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n4xrKg0V_JtA"
   },
   "outputs": [],
   "source": [
    "# 2. Apply vader_compound_score on data. New column name would be 'vader_sent'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8A7CR4l9_JtA"
   },
   "outputs": [],
   "source": [
    "# 3. Let's draw scatter plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86IxwLUC_JtA"
   },
   "source": [
    " 4. Enter your conclusion below based on the scatter plot:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i1M3oOXD_JtA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AFdgo5KRJzSP"
   },
   "source": [
    "# Sentiment analysis for each review\n",
    "\n",
    "Let's apply our sentiment methods for individual reviews. \n",
    "\n",
    "To do so, we will read our original restaurant review data, which has individual reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "executionInfo": {
     "elapsed": 1068,
     "status": "ok",
     "timestamp": 1612159678040,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "hwGkSukAJzSP",
    "outputId": "88975812-863b-44a6-ac6c-78d53b510ddb"
   },
   "outputs": [],
   "source": [
    "df = pd.read_table(\"https://raw.githubusercontent.com/anjisun221/css_codes/main/restaurant_reviews.tsv\", sep=\"\\t\")\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 817,
     "status": "ok",
     "timestamp": 1612159679656,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "Qo4jw6omJzSP",
    "outputId": "92e32f39-08a5-4c45-e48e-e6dfda9d4316"
   },
   "outputs": [],
   "source": [
    "# Let's add category label\n",
    "\n",
    "df['category'] = df['aspect']+\"_\"+df['sentiment']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    },
    "executionInfo": {
     "elapsed": 3267,
     "status": "ok",
     "timestamp": 1612159683736,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "KRUANlGdJzSP",
    "outputId": "b0cfec2f-ed30-495e-be15-86ef98320fbe"
   },
   "outputs": [],
   "source": [
    "#  Let's compute polarity and subjectivity scores for each review using TextBlob\n",
    "\n",
    "df['polarity'] = df['text'].apply(blob_polarity)\n",
    "df['subjectivity'] = df['text'].apply(blob_subjectivity)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "executionInfo": {
     "elapsed": 1669,
     "status": "ok",
     "timestamp": 1612159689560,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "lvldGGDuJzSP",
    "outputId": "b1b20e1e-bd1b-4240-824f-09e60d1849a4"
   },
   "outputs": [],
   "source": [
    "# We draw box plot and strip plot to examine the distribution of \n",
    "# polarity scores by TextBlob for each review in each category. \n",
    "\n",
    "# For visualization, we will use SeaBorn python library. \n",
    "import seaborn as sns\n",
    "\n",
    "# Initialize the figure\n",
    "f, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Plot the polarity with horizontal boxes\n",
    "sns.boxplot(x=\"polarity\", y=\"category\", data=df,\n",
    "            whis=[0, 100], width=.6, palette=\"Set2\")\n",
    "\n",
    "# Add in points to show each observation\n",
    "sns.stripplot(x=\"polarity\", y=\"category\", data=df,\n",
    "              size=4, linewidth=0)\n",
    "\n",
    "# Tweak the visual presentation\n",
    "ax.xaxis.grid(True)\n",
    "ax.set(ylabel=\"\")\n",
    "sns.despine(trim=True, left=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ihKmGDfaJzSP"
   },
   "source": [
    "## Exercise 4\n",
    "\n",
    "Let's compare the sentiments of individual reviews computed by Textblob and Vader\n",
    "\n",
    "You will need to apply Vader to analyze sentiment of reviews. \n",
    "\n",
    "1. Use the function that return compound score given a sentence defined in Exercise 3\n",
    "2. Apply function in 1. to compute vader score for each of reviews, the column name would be 'vader_sent' \n",
    "3. Draw a box+strip plot for Vader score and compare the results of the two methods\n",
    "4. What's your conclusion?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XI5_D7vTJzSP"
   },
   "outputs": [],
   "source": [
    "# 1. We assume that you have defined function called vader_compount_score. \n",
    "#  Otherwise, please uncomment below and wirte your code.\n",
    "\n",
    "# def vader_compound_score(sentence):     \n",
    "    #[Enter your code]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dYGZXCyvJzSQ"
   },
   "outputs": [],
   "source": [
    "# 2. Apply vader_compound_score on data. New column name would be 'vader_sent'\n",
    "\n",
    "df['vader_sent'] = df['text'].apply(vader_compound_score)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rwbvYzslJzSQ"
   },
   "outputs": [],
   "source": [
    "# 3. Draw box plot and strip plot to examine the distribution of vader scores for each review in each category. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QlEueFKGJzSQ"
   },
   "outputs": [],
   "source": [
    "# 4. What's your conclusion?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ulvcUecJzSQ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab4_sentiment_analysis.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
