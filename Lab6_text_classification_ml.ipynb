{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6 - Text classification by machine learning\n",
    "\n",
    "In this lab, you will learn:\n",
    "* How to use machine learning model to classify text\n",
    "* How to evaluate the performance of different models\n",
    "\n",
    "This lab is written by Jisun AN (jisunan@smu.edu.sg) and Michelle KAN (michellekan@smu.edu.sg).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages for data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Packages for machine learning models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# Packages for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Getting the data\n",
    "\n",
    "In this lab, we will use restaurant review data. \n",
    "\n",
    "This data is manually annotated by humans according to their aspect and sentiment. \n",
    "\n",
    "One review may have two or more aspects and thus two or more sentiment. \n",
    "\n",
    "We note that we excluded those conflicting reviews.\n",
    "\n",
    "\"restaurant_reviews.tsv\" is tab-separated file which fields are: \n",
    "\n",
    "- `sid` is review id\n",
    "- `text` is a review\n",
    "- `aspect` refers to the review area of interest. It consists of any of these five labels: <i>food, service, ambience, price</i> \n",
    "- `sentiment` consists of one of these labels: <i>positive, negative, neutral</i>\n",
    "\n",
    "\n",
    "From this dataset, we will create **a 'balanced' dataset** to build classification models. \n",
    "\n",
    "The balanced dataset includes the equal number of samples of each label. \n",
    "\n",
    "**We will sample 500 positive texts and 500 negative texts.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_df = pd.read_table(\"https://raw.githubusercontent.com/anjisun221/css_codes/main/restaurant_reviews.tsv\", sep=\"\\t\")\n",
    "print(ori_df.shape)\n",
    "ori_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_df['text'][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_df['sentiment'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 500 rows from dataframe --> sample 500 positive texts.\n",
    "df_pos = ori_df.query('sentiment == \"positive\"').sample(500, random_state=999)\n",
    "df_pos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 500 rows from dataframe --> sample 500 negative texts.\n",
    "df_neg = ori_df.query('sentiment == \"negative\"').sample(500, random_state=999)\n",
    "df_neg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine two dataframes \n",
    "df = pd.concat([df_pos, df_neg])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We extract text and label to build the model\n",
    "\n",
    "sentences = df['text'].values\n",
    "y = df['sentiment'].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) is a function in Sklearn model selection for splitting data arrays into two subsets: for training data and for testing data. With this function, you don't need to divide the dataset manually. It has the following syntax:\n",
    "\n",
    "    train_test_split(X, y, train_size=0.*,test_size=0.*, random_state=*)\n",
    "\n",
    "The function takes the following parameters:\n",
    "- `X, y`: the dataset you're selecting to use. Allowed inputs are lists, numpy arrays, scipy-sparse matrices or pandas dataframes.\n",
    "- `train_size`: This parameter sets the size of the training dataset. There are three options: None, which is the default, Int, which requires the exact number of samples, and float, which ranges from 0.1 to 1.0.\n",
    "- `test_size`: This parameter specifies the size of the testing dataset. The default state suits the training size. It will be set to 0.25 if the training size is set to default.\n",
    "- `random_state`: The default mode performs a random split using `np.random`. Alternatively, you can add an integer using an exact number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly split the data into training (80%) and test (20%) datasets\n",
    "sentences_train, sentences_test, y_train_str, y_test_str = train_test_split(sentences, y, test_size=0.20, random_state=999)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Extract features (words are features)\n",
    "\n",
    "### Document-Term Matrix\n",
    "\n",
    "For many of the techniques we'll be using in future notebooks, the text must be tokenized, meaning broken down into smaller pieces. The most common tokenization technique is to break down text into words. We can do this using scikit-learn's [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html), where every row will represent a different document and every column will represent a different word.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to create a document-term matrix using CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(sentences_train)\n",
    "X_train = vectorizer.transform(sentences_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encode the target variable - this will change our string labels to integer labels. \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train_str)\n",
    "y_test = encoder.fit_transform(y_test_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame({'y':y_train})\n",
    "tmp['y'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame({'y':y_test})\n",
    "tmp['y'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1. Improve Document-Term Matrix (DTM)\n",
    "\n",
    "You can improve the performance of the classification models by having better or other features. \n",
    "In text classification, this can be done by, for example, excluding common English stop words or adding bigrams.\n",
    "You can do it by adding some parameters of scikit-learn's [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). \n",
    "\n",
    "Challenge: Remove stop words and add bigram in the DTM and see whether the performance of the model improves. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = #[WRITE YOUR CODE]\n",
    "vectorizer.fit(sentences_train)\n",
    "X_train = vectorizer.transform(sentences_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Build the model and evaluate via cross-validation\n",
    "\n",
    "\n",
    "We will use two classification algorithms. \n",
    "* [Naïve Bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html): a family of probabilistic algorithms that uses Bayes’s Theorem to predict the category of a text.\n",
    "* [Support Vector Machines](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html): a non-probabilistic model which uses a representation of text examples as points in a multidimensional space. Examples of different categories (sentiments) are mapped to distinct regions within that space. Then, new texts are assigned a category based on similarities with existing texts and the regions they’re mapped to.\n",
    "\n",
    "Cross-validation is a common method to evaluate the performance of a text classifier. It works by splitting the training dataset into random, equal-length example sets (e.g., 4 sets with 25% of the data). For each set, a text classifier is trained with the remaining samples (e.g., 75% of the samples). Next, the classifiers make predictions on their respective sets, and the results are compared against the human-annotated tags. This will determine when a prediction was right (true positives and true negatives) and when it made a mistake (false positives, false negatives).\n",
    "\n",
    "We will use [sklearn's Cross-validation](https://scikit-learn.org/stable/modules/cross_validation.html) function to implement it.\n",
    "\n",
    "With these results, you can build performance metrics that are useful for a quick assessment on how well a classifier works:\n",
    "\n",
    "* Accuracy: the percentage of texts that were categorized with the correct tag.\n",
    "* Precision: the percentage of examples the classifier got right out of the total number of examples that it predicted for a given tag.\n",
    "* Recall: the percentage of examples the classifier predicted for a given tag out of the total number of examples it should have predicted for that given tag.\n",
    "* F1 Score: the harmonic mean of precision and recall.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation metrics we want to get from cross validation\n",
    "scoring = ['precision_macro', 'recall_macro', 'f1_macro', 'accuracy', 'balanced_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the mean values of evaluation metrics across 5 experiments \n",
    "def print_cross_validation_result(cross_val_result):\n",
    "    print(\"Cross Accuracy : \",round(cross_val_result['test_accuracy'].mean() * 100 , 2),\"%\")\n",
    "    print(\"Cross Validation Precision : \",round(cross_val_result['test_precision_macro'].mean() * 100 , 2),\"%\")\n",
    "    print(\"Cross Validation Recall : \",round(cross_val_result['test_recall_macro'].mean() * 100 , 2),\"%\")\n",
    "    print(\"Cross Validation F1 : \",round(cross_val_result['test_f1_macro'].mean() * 100 , 2),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Naive Bayes --- \")\n",
    "cross_val_naive = cross_validate(estimator = MultinomialNB(), X = X_train, y = y_train, scoring=scoring, cv = 5, n_jobs = -1)\n",
    "print_cross_validation_result(cross_val_naive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Linear SVM results --- \")\n",
    "cross_val_svc_linear = cross_validate(estimator = SVC(kernel='linear'), X = X_train, y = y_train, scoring=scoring, cv = 5, n_jobs = -1)\n",
    "print_cross_validation_result(cross_val_svc_linear)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2. Let's change the parameters of SVM to improve the performance. \n",
    "\n",
    "[sklearn's SVM document](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)\n",
    "\n",
    "**SVM's C parameter:** The C parameter allows you to decide how much you want to penalize misclassified points.\n",
    "\n",
    "**SVM's Kernel:** You can use various kernels of SVM. You can specify the kernel type to be used in the algorithm by using 'kernel' parameber. It must be one of ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, or ‘precomputed’\n",
    "\n",
    "Try with various C and Kernel and find the parameters with the best performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SVM results --- \")\n",
    "cross_val_svc_linear_2 = # WRTIE YOUR CODE\n",
    "\n",
    "print_cross_validation_result(cross_val_svc_linear_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3. Let's build the model using Random forest. \n",
    "\n",
    "You can find the example here:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "You will need to import the necessary library, and then change the 'estimator' to the one for random forest. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WRITE-YOUR-CODE # import the library \n",
    "\n",
    "print(\"Ramdom Forest --- \")\n",
    "cross_val_rfc = #WRITE-YOUR-CODE\n",
    "print_cross_validation_result(cross_val_rfc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Find the most important features\n",
    "\n",
    "We can visualize the most important features in classifying texts into either positive or negative review.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_coefficients(classifier, feature_names, modelname, top_features=20):\n",
    "    coef = classifier.coef_.ravel() \n",
    "    top_positive_coefficients = np.argsort(coef)[-top_features:]\n",
    "    top_negative_coefficients = np.argsort(coef)[:top_features]\n",
    "    top_coefficients = np.hstack([top_negative_coefficients, top_positive_coefficients])\n",
    "    # create plot\n",
    "    plt.figure(figsize=(16, 6))\n",
    "\n",
    "    plt.title('Important features by %s model' % (modelname), fontsize=20)\n",
    "    plt.ylabel('Coefficient', fontsize=18)\n",
    "    plt.xlabel('Negative Reviews <<------------------ Important features ------------------>> Positive Reviews', fontsize=18)\n",
    "    \n",
    "    colors = ['red' if c < 0 else 'blue' for c in coef[top_coefficients]]\n",
    "    plt.bar(np.arange(2 * top_features), coef[top_coefficients], color=colors)\n",
    "    feature_names = np.array(feature_names)\n",
    "    plt.xticks(np.arange(0, 0 + 2 * top_features), feature_names[top_coefficients], rotation=60, ha='right', fontsize=14)    \n",
    "\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We build the model using all our training data\n",
    "svm = LinearSVC() # this is another way to define SVM model with linear kernel. we need to use this to see the important features. \n",
    "svm.fit(X_train, y_train)\n",
    "plot_coefficients(svm, vectorizer.get_feature_names(), \"Linear SVM\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We build the model using all our training data\n",
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes.fit(X_train,y_train)\n",
    "plot_coefficients(naive_bayes, vectorizer.get_feature_names(), \"Naive Bayes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Classify new texts into positive or negative class & find the best model\n",
    "\n",
    "Using our test dataset, we will find the best model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build the best performed models\n",
    "m_naive = MultinomialNB().fit(X_train, y_train)\n",
    "m_svm = SVC(kernel='linear').fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to extract features for our test set, and built DTM for test texts\n",
    "X_test = vectorizer.transform(sentences_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_and_evaluate(mymodel, X_test):\n",
    "    predicted = mymodel.predict(X_test)\n",
    "    y_true = y_test\n",
    "    y_pred = predicted\n",
    "    print(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"naive bayes ---\")\n",
    "classify_and_evaluate(m_naive, X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SBM Linear --- \")\n",
    "classify_and_evaluate(m_svm, X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4. Classify the below new texts!\n",
    "\n",
    "You have two new texts. Please use the best model to classify those texts into positive or negative. \n",
    "Print out the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_texts = [\"Authentic, cheap, huge portion Korean food in orchard. \", \n",
    "             \"The food is server quite fast but compare to the quantity of tender beef given last time & now is like a reduction in size.\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "[Write your code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced exercise (optional) \n",
    "\n",
    "Instead of CountVectorizer, you can consider to use TFIDF vectorizer to improve the performance of the model.\n",
    "See details about TFIDF vectorization [here](https://medium.com/@cmukesh8688/tf-idf-vectorizer-scikit-learn-dbc0244a911a) and see sklearn's [TFIDF vectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html).\n",
    "\n",
    "Define TFIDF vectorizer and use it as a replacement of CountVectorizer and see whether it improves the performance of the classification model. You can rerun from the Section 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = # WRITE YOUR CODE\n",
    "vectorizer.fit(sentences_train)\n",
    "X_train = vectorizer.transform(sentences_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
