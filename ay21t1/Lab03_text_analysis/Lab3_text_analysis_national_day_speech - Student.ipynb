{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/anjisun221/css_codes/blob/main/ay21t1/Lab03_text_analysis/Lab3_text_analysis_national_day_speech.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pnxbJzbv_Jsr"
   },
   "source": [
    "# Lab 3 - Text-as-Data, National Day Speech analysis\n",
    "\n",
    "In this lab, you will learn:\n",
    "* How to clean texts\n",
    "* How to count words from texts and draw wordcloud from word frequencies\n",
    "* How to do comparative text analysis by extracting representative words from a corpus using lod-odds analysis\n",
    "\n",
    "This lab is written by Jisun AN (jisunan@smu.edu.sg) and Michelle KAN (michellekan@smu.edu.sg).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Google Drive as an accessible path (Optional if you are running from Jupyter Notebook)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# change path to the designated google drive folder\n",
    "# otherwise, data will be saved in /content folder which you may have issue locating\n",
    "%cd /content/drive/My Drive/Colab Notebooks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mypath = \".\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. import utility functions\n",
    "\n",
    "#### We've created a few utility functions. \n",
    "\n",
    "```read_word_count_file()``` : the input is csv file and the output is dictionary of word counts where the key is word and the value is the frequency of the words. e.g., {'singapore': 6472873} The input file must be a CSV file where each line is a pair of word and its counrt (e.g., singapore, 6472873).  \n",
    "\n",
    "\n",
    "```read_word_count_file_online()``` : this helps to import the data from online file. The input is url of the csv file and the output is dictionary of word counts where the key is word and the value is the frequency of the words. e.g., {'singapore': 6472873} The input file must be a CSV file where each line is a pair of word and its counrt (e.g., singapore, 6472873).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's download ```smt203util.py``` Below code should download the file in the same folder where your jupyter notebook is. \n",
    "!wget https://raw.githubusercontent.com/anjisun221/css_codes/main/ay21t1/Lab03_text_analysis/smt203util.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's import all function from smt203util! \n",
    "from smt203util import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VBe-9KkA_Jsw"
   },
   "source": [
    "# 1. Getting the data\n",
    "\n",
    "In this lab, we will use national day speech data. \n",
    "\n",
    "This data is manually annotated by humans according to their aspect and sentiment. \n",
    "\n",
    "One review may have two or more aspects and thus two ore more sentiment. \n",
    "\n",
    "We note that we excluded those conflicting reviews.\n",
    "\n",
    "\"restaurant_reviews.tsv\" is tab-separated file which fields are: \n",
    "\n",
    "- `sid` is review id\n",
    "- `text` is a review\n",
    "- `aspect` refers to the review area of interest. It consists of any of these five labels: <i>food, service, ambience, price</i> \n",
    "- `sentiment` consists of one of these labels: <i>positive, negative, neutral</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2136,
     "status": "ok",
     "timestamp": 1612159073697,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "7Yig1Syu_Jsx"
   },
   "outputs": [],
   "source": [
    "### Import Pandas to analyze the data\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read the file using Pandas 'read_table' function (either read_table, read_csv is fine)\n",
    "df = pd.read_table(\"https://raw.githubusercontent.com/anjisun221/css_codes/main/ay21t1/Lab03_text_analysis/national_day_speech_1996_1991_2004_2020.tsv\", sep=\"\\t\")\n",
    "# df = pd.read_table(\"./national_day_speech_1996_1991_2004_2020_extended.tsv\", sep=\"\\t\")\n",
    "\n",
    "### print the size of data frame (data frame == table)\n",
    "print(df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### to see the entire text \n",
    "pd.set_option('display.max_colwidth', 150)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.speech.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['speech'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "79d-nz2a_Js0"
   },
   "source": [
    "# 2. Cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "orQH1Z1P_Js0"
   },
   "source": [
    "When dealing with numerical data, data cleaning often involves removing null values and duplicate data, dealing with outliers, etc. With text data, there are some common data cleaning techniques, which are also known as text pre-processing techniques.\n",
    "\n",
    "With text data, this cleaning process can go on forever. There's always an exception to every cleaning step. So, we're going to follow the MVP (minimum viable product) approach - start simple and iterate. Here are a bunch of things you can do to clean your data. We're going to execute just the common cleaning steps here and the rest can be done at a later point to improve our results.\n",
    "\n",
    "**Common data cleaning steps on all text:**\n",
    "* Make text all lower case\n",
    "* Remove punctuation\n",
    "* Remove numerical values\n",
    "* Remove common non-sensical text (/n)\n",
    "* Tokenize text\n",
    "* Remove stop words\n",
    "\n",
    "**More data cleaning steps after tokenization:**\n",
    "* Stemming / lemmatization\n",
    "* Parts of speech tagging\n",
    "* Create bi-grams or tri-grams\n",
    "* Deal with typos\n",
    "* And more..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "egoiXlSq_Js0"
   },
   "source": [
    "## Round 1. Let's convert text to lowercase, remove punctuations, remove words containing numbers.\n",
    "\n",
    "Python has a built-in package called `re`, which can be used to work with Regular Expressions. Regular Expression, is a sequence of characters that forms a search pattern.<br>\n",
    "\n",
    "The `re.sub()` function can be used to replace substrings. The syntax `re.sub(pattern,repl,text)` replaces the pattern matches in text with repl. In the following code, it is used to remove punctuations and remove words containing number. You can read up about `re.sub()` [here](https://www.w3schools.com/python/python_regex.asp) and [here](https://www.pythonforbeginners.com/regex/regular-expressions-in-python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 803,
     "status": "ok",
     "timestamp": 1612159370922,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "JSKxJUWu_Js1"
   },
   "outputs": [],
   "source": [
    "# Apply a first round of text cleaning techniques\n",
    "import re \n",
    "import string\n",
    "\n",
    "def clean_text_round1(text):\n",
    "    '''Make text lowercase, remove punctuation and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 896,
     "status": "ok",
     "timestamp": 1612159378206,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "SF4n0-7Z_Js1",
    "outputId": "0a72538c-4ed8-4a85-8462-404b424c29e6"
   },
   "outputs": [],
   "source": [
    "# Let's take a look at the updated text\n",
    "df['clean_text_1'] = df['text'].apply(clean_text_round1)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dn6CSU54JzSG"
   },
   "source": [
    "## Additional exercise 1 (optional)\n",
    "\n",
    "1. Get more familar with regular expression below. \n",
    "\n",
    "2. Can you remove url from a tweet using regular expression? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OqzpLSMhJzSG"
   },
   "outputs": [],
   "source": [
    "# Examples of Regular expression \n",
    "import re # Need module 're' for regular expression\n",
    "\n",
    "# Try find: re.findall(regexStr, inStr) -> matchedSubstringsList\n",
    "# r'...' denotes raw strings which ignore escape code, i.e., r'\\n' is '\\'+'n'\n",
    "# [0-9] matches any digit; [A-Za-z] matches any uppercase or lowercase letters.\n",
    "# + means one or more\n",
    "print(re.findall(r'[0-9]+', 'abc123xyz')) # Return a list of matched substrings.  \n",
    "print(re.findall(r'[0-9]+', 'abcxyz')) # Return []\n",
    "print(re.findall(r'[0-9]+', 'abc00123xyz456_0')) # Return ['00123', '456', '0']\n",
    "\n",
    "\n",
    "# Try substitute: re.sub(regexStr, replacementStr, inStr) -> outStr\n",
    "# Below code will replace all number block to *\n",
    "print(re.sub(r'[0-9]+', r'*', 'abc00123xyz456_0')) # Return 'abc*xyz*_*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OZLVtTHcJzSG"
   },
   "outputs": [],
   "source": [
    "# Here's an example tweet\n",
    "mytweet = \"New pre-print that @GruppiMauricio, @sibel_adali and I have been holding on to for a while: https://arxiv.org/abs/2101.10973. The goal was to leverage content sharing practices by news outlets in news veracity detection. Thread.\"\n",
    "mytweet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3vVAY8PqJzSH"
   },
   "source": [
    "Write the code to remove URL from the above tweet.\n",
    "\n",
    "As a result, you should see \n",
    "\n",
    "'New pre-print that @GruppiMauricio, @sibel_adali and I have been holding on to for a while:  The goal was to leverage content sharing practices by news outlets in news veracity detection. Thread.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = # Write your code \n",
    "text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BPvxSGDd_Js1"
   },
   "source": [
    "## Round 2. Let's remove stopwords. \n",
    "\n",
    "A stop word is a commonly used word (such as \"the\", \"a\", \"an\", \"in\"). For some analysis, like looking into top words, those stop words are often meaningless, and thus we remove them.\n",
    "\n",
    "The [Natural Language Toolkit (nltk)](https://www.nltk.org/api/nltk.html) is a Python package for natural language processing. We will import the library for the removal of stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2852,
     "status": "ok",
     "timestamp": 1612159387226,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "0IG3CWjb_Js2",
    "outputId": "05480210-ef61-4d7c-c55c-00f62e3b2095"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UmiMDOChJzSI"
   },
   "source": [
    "The `nltk` library has a list of stopwords stored in 16 different languages. We will retrieve the list of English stop words using `stopwords.words('english')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 969,
     "status": "ok",
     "timestamp": 1612159389877,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "UhKTgIXoJzSI"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# Exclude stopwords with Python's list comprehension and pandas.DataFrame.apply.\n",
    "def clean_text_round2(text):\n",
    "    return ' '.join([word for word in text.split() if word not in (stop)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JWNTnZ2rJzSI"
   },
   "source": [
    "The expression `[word for word in x.split() if word not in (stop)]` is a list comprehension.<br>\n",
    "List comprehension offers a shorter syntax when you want to create a new list based on the values of an existing list.<br> Syntax: `new_list = [expression for item in iterable list if condition == True]`<br> \n",
    "You can read up about list comprehension [here](https://www.w3schools.com/python/python_lists_comprehension.asp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 830,
     "status": "ok",
     "timestamp": 1612159393241,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "gPXZ2o6k_Js2",
    "outputId": "80caadaf-a88b-441b-bccb-8da9882dc458"
   },
   "outputs": [],
   "source": [
    "df['clean_text_2'] = df['clean_text_1'].apply(clean_text_round2)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p4HjENg8_Js2"
   },
   "source": [
    "**NOTE:** This data cleaning aka text pre-processing step could go on for a while, but we are going to stop for now. After going through some analysis techniques, if you see that the results don't make sense or could be improved, you can come back and make more edits such as:\n",
    "* Mark 'outstanding' and 'outstand' as the same word (stemming / lemmatization)\n",
    "* Combine 'thank you' into one term (bi-grams)\n",
    "* And a lot more..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Unigram analysis - Counting words (and save it to a file) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create ```word_counts``` folder \n",
    "import os\n",
    "os.makedirs('word_counts', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function create a dictionary with word counts from dataframe \n",
    "def count_words_from_dataframe(df):\n",
    "    result_dict = {}\n",
    "    # iterate rows of dataframe \n",
    "    for index, row in df.iterrows():\n",
    "        text = row['clean_text_2']\n",
    "        \n",
    "        # this will split a sentence into words \n",
    "        tokens = text.split()\n",
    "        \n",
    "        # iterate each word \n",
    "        for i in range(0, len(tokens)):\n",
    "            token = tokens[i]\n",
    "            try:\n",
    "                result_dict[token] += 1\n",
    "            except KeyError:\n",
    "                result_dict[token] = 1\n",
    "                    \n",
    "    return result_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_speeches = list(df.speech.unique())\n",
    "list_of_speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "count_type = 'unigram'\n",
    "\n",
    "for each_speech in list_of_speeches:\n",
    "    print(each_speech)\n",
    "    \n",
    "    ## query helps to filter rows of dataframe given a condition \n",
    "    df_speech = df.query('speech == @each_speech')\n",
    "    print(df_speech.shape)\n",
    "    \n",
    "    ## this function will return a dictionary of words and frequency\n",
    "    result = count_words_from_dataframe(df_speech)\n",
    "\n",
    "    ## soring the words based on their frequency\n",
    "    sorted_dic = ((k, result[k]) for k in sorted(result, key=result.get, reverse=True))\n",
    "    \n",
    "    ## write the dictionary in a file\n",
    "    with open(f\"./word_counts/{each_speech}_{count_type}_counts.csv\", 'w') as fp:\n",
    "        writer = csv.writer(fp, delimiter=',')\n",
    "        writer.writerows(sorted_dic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's draw WordCloud using unigrams!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This it OPTIONAL if you are running the current notebook using Google Colab\n",
    "!conda install --yes -c conda-forge wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import relevant libraries\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "from PIL import Image\n",
    "from os import path\n",
    "import csv\n",
    "\n",
    "### this function will creat wordcloud based on word frequencies and save them into files under plot. \n",
    "def makeImage(termDict, outputfile):\n",
    "    \n",
    "    wc = WordCloud(max_font_size=60, width=1280, height=720, background_color=\"white\")    \n",
    "\n",
    "    ### generate word cloud using frequencies!\n",
    "    wc.generate_from_frequencies(termDict)\n",
    "    wc.to_file(outputfile+\".png\")\n",
    "    wc.to_file(outputfile+\".pdf\")\n",
    "    \n",
    "    ### show the figure\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show() \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create ```plot``` folder \n",
    "os.makedirs('plot', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_name = \"2004_lee_hsien_loong\"\n",
    "### We're using one of the utility function! \"read_word_count_file\"\n",
    "fullTermsDict = read_word_count_file(f'{mypath}/word_counts/{speech_name}_unigram_counts.csv')\n",
    "outputfile = f\"./plot/wordcloud_count_unigram_{speech_name}\"\n",
    "makeImage(fullTermsDict, outputfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "speech_name = \"2020_lee_hsien_loong\"\n",
    "### We're using one of the utility function! \"read_word_count_file\"\n",
    "fullTermsDict = read_word_count_file(f'{mypath}/word_counts/{speech_name}_unigram_counts.csv')\n",
    "outputfile = f\"./plot/wordcloud_count_unigram_{speech_name}\"\n",
    "makeImage(fullTermsDict, outputfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1. Count bigram \n",
    "\n",
    "You task is to write a function named ```count_bigrams_from_dataframe``` whose input is dataframe, ```df``` and output is the dictionary of bigrams with their counts. \n",
    "\n",
    "Then, can you also save them into file after sorting the bigrams based on their counts? \n",
    "\n",
    "(Optional) You may draw word clouds with bigrams as well! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_bigrams_from_dataframe(df):\n",
    "    \n",
    "    result_dict = {}                    \n",
    "    for index, row in df_speech.iterrows():\n",
    "        text = row['clean_text_2']\n",
    "        tokens = text.split()\n",
    "        \n",
    "        ## write your code \n",
    "        \n",
    "    return result_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_type = 'bigram'\n",
    "\n",
    "for each_speech in list_of_speeches:\n",
    "    print(each_speech)\n",
    "    df_speech = df.query('speech == @each_speech')\n",
    "    print(df_speech.shape)\n",
    "    \n",
    "    ## write your code \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Find representative words by using log odds \n",
    "\n",
    "### In this example, we will compare 2004 and 2020 National Day speeches by PM Lee Hsien Loong. \n",
    "\n",
    "We will detect unigrams that are over-represented in either 2004 or 2020 speech. \n",
    "\n",
    "To do so, we use log-odds ratios with informative Dirichlet priors (Monroe, Colaresi, and Quinn 2008). This method estimates the log-odds ratio of each word between two corpora i and j given the prior frequencies obtained from a back ground corpus. \n",
    "\n",
    "This method identifies representative words of a corpus i in comparison with another corpus j and the common corpus (global/background corpus). \n",
    "\n",
    "#### We've created a few functions for computing log odds values and finding the representativ words.\n",
    "\n",
    "```calculate_log_odds_idp(global_counts, counts_i_name, counts_i_dict, counts_j_name, counts_j_dict)``` : it returns the log odds value (```log_odds_z_score```) for each word, which indicates the significance of the word in the corresponding corpus. You should provide background corpus (we call ```global_counts```), two names of the corpora: ```counts_i_name``` and ```counts_j_name``` (for naming the file), and two dictionaries of word counts from each corpus: ```counts_i_dict``` and ```counts_j_dict```. \n",
    "\n",
    "```find_discriminative_words(top_words_df, threshold_i, threshold_j, num_i, num_j, mypath)```: This function will filter out those words that appear less than k times in each corpus (`threshold_i` and ```threshold_j```), rank the words based on lod-odds z-scores and select top-N representative words (```num_i```, ```num_j```), and generate a csv file where each row contains (word, log_odds_z_score) for each of the two corpora .  \n",
    "\n",
    "For details about how to compute log odds z-score, see: \"Monroe, B. L.; Colaresi, M. P.; and Quinn, K. M. 2008. Fightin’words: Lexical feature selection and evaluation for identifying the content of political conflict. Political Analysis 16(4):372–403.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Getting global word counts (frequency of unigram in Google News datasets)\n",
    "target_url = \"https://raw.githubusercontent.com/anjisun221/css_codes/main/ay21t1/Lab03_text_analysis/1gram_englishall_count.csv\"\n",
    "global_counts = read_word_count_file_online(target_url)\n",
    "print(\"The number of unigrams=\", len(global_counts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Removing stopwords from the word list and \n",
    "stopwords_list = get_stopwords()\n",
    "global_counts = {k: v for k, v in global_counts.items() if (k not in stopwords_list) and (len(k) > 2)}\n",
    "print(\"After excluding stop words: \", len(global_counts))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's load word frequency file and create dictionary of word counts for the two speeches!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts_i_name = \"1966_lee_kuan_yew\"\n",
    "# counts_i_name = \"1991_goh_chok_tong\"\n",
    "counts_i_name = \"2004_lee_hsien_loong\"\n",
    "# counts_j_name = \"2020_lee_hsien_loong\"\n",
    "counts_i = read_word_count_file(f\"{mypath}/word_counts/{counts_i_name}_unigram_counts.csv\")\n",
    "\n",
    "### we filter out words that exist in our background corpus\n",
    "counts_i_dict = {k: v for k, v in counts_i.items() if k in global_counts}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts_j_name = \"1966_lee_kuan_yew\"\n",
    "# counts_j_name = \"1991_goh_chok_tong\"\n",
    "# counts_j_name = \"2004_lee_hsien_loong\"\n",
    "counts_j_name = \"2020_lee_hsien_loong\"\n",
    "counts_j = read_word_count_file(f\"{mypath}/word_counts/{counts_j_name}_unigram_counts.csv\")\n",
    "\n",
    "### we filter out words that exist in our background corpus\n",
    "counts_j_dict = {k: v for k, v in counts_j.items() if k in global_counts}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### this function will return log-odds values. \n",
    "top_words_df = calculate_log_odds_idp(global_counts, counts_i_name, counts_i_dict, counts_j_name, counts_j_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation lad z-score (log_odds_z_score) \n",
    "\n",
    "Consider it as a value that indicate the significance of the word in a corpus. Given that we have two corpora, i and j, the order matters when interpreting this z-score. For corpus i, the lower the z-score, more significant the word is while for corpus j, the higher the z-score, more significant the word is.\n",
    "\n",
    "The word frequency is also important, if the word frequency is low, those words may not be meaningful. So, we filter words based on the two condition: word frequency and z-score. \n",
    "\n",
    "In the below example, \n",
    "for corpus i, we will consider words that have lower z-scores and that appear at least twice in the corpus. \n",
    "\n",
    "For corpus j, we will consider words that have higher z-scores and that appear at least twice in the corpus. \n",
    "\n",
    "Note that, we choose 2 as a threshold for word frequency, but usually it should be much higher if we have larger corpus. Our speech data is small, so we have to choose 2. Normally, we would use 50 or 100 as a minimum number of word frequency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Below shows the representative words of 2004 National Day speech by PM Lee. \n",
    "top_words_df[top_words_df[counts_i_name] >= 2].iloc[::-1].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Below shows the representative words of 2020 National Day speech by PM Lee. \n",
    "top_words_df[top_words_df[counts_j_name] >= 2].head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Below function will simply help you to print the above table into file, so that we can use it for drawing word cloud. \n",
    "### The output of this function is a csv file where each row contains (word, log_odds_z_score) for the two corpora. \n",
    "### threshold_i and threshold_j are the threshold of word frequency. \n",
    "### num_i and num_j are the number of representative words it will write in file. \n",
    "find_discriminative_words(top_words_df, threshold_i=2, threshold_j=2, num_i=20, num_j=20, mypath='.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the above function (`find_discriminative_words()`) results in creating two files where each row is a word and its log-odds value and each file is for each speech!!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's draw WordClouds based on log odds values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2. Draw WordClouds based on the log odds values for each speech and write the WordClouds in a file. \n",
    "\n",
    "You can use the resulting files of `find_discriminative_words()` to draw the wordcloud. \n",
    "\n",
    "Hint 1: You can simply consider \"log-odds value\" as a frequency. \n",
    "Hint 2: Note that you need to create one wordcloud for each speech. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Draw a WordCloud for 2004 National Day speech by PM Lee.\n",
    "\n",
    "[Write your code] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw a WordCloud for 2020 National Day speech by PM Lee. \n",
    "\n",
    "[Write your code] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab4_sentiment_analysis.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
