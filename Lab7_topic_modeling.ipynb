{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ncL7zRE_1a4T"
   },
   "source": [
    "# Lab 7 - Topic Modeling\n",
    "\n",
    "In this lab, you will learn:\n",
    "* How to find topics in a corpus using topic modeling\n",
    "* How to apply Latent Dirichlet Allocation (LDA), a topic modeling technique, to texts\n",
    "* How to find the distribution of LDA topics in a corpus\n",
    "\n",
    "This lab is written by Jisun AN (jisunan@smu.edu.sg) and Michelle KAN (michellekan@smu.edu.sg) based on existing tutorial, titled \"[Topic Modeling with Gensim (Python)](https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/),\" by Selva Prabhakaran. \n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LLKFXxOW1a4Y"
   },
   "source": [
    "# 0. Introduction \n",
    "\n",
    "One of the primary applications of natural language processing is to automatically extract what topics people are discussing from large volumes of text. Some examples of large text could be feeds from social media, customer reviews of hotels, movies, etc, user feedbacks, news stories, e-mails of customer complaints etc.\n",
    "\n",
    "Knowing what people are talking about and understanding their problems and opinions is highly valuable to businesses, administrators, political campaigns. And it’s really hard to manually read through such large volumes and compile the topics.\n",
    "\n",
    "Thus is required an automated algorithm that can read through the text documents and automatically output the topics discussed. This is called topic modeling. The ultimate goal of topic modeling is to find various topics that are present in your corpus. Each document in the corpus will be made up of at least one topic, if not multiple topics.\n",
    "\n",
    "In this tutorial, we will take a real example of the COVID-19 Twitter dataset and use **Latent Dirichlet Allocation (LDA)**, which is one of many topic modeling techniques. It was specifically designed for text data, to extract the naturally discussed topics.\n",
    "\n",
    "We will also extract the volume and percentage contribution of each topic to get an idea of how important a topic is.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 565
    },
    "executionInfo": {
     "elapsed": 10255,
     "status": "ok",
     "timestamp": 1616415994697,
     "user": {
      "displayName": "Jisun AN",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "kxTJBgnB1a4Y",
    "outputId": "00a846fe-81ab-4abc-d86e-a627bf709393"
   },
   "outputs": [],
   "source": [
    "!pip install pyldavis\n",
    "!pip install -U gensim\n",
    "# !pip install en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZcmZSfyP5ytl"
   },
   "source": [
    "You must restart runtime after updating libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2122,
     "status": "ok",
     "timestamp": 1616416045617,
     "user": {
      "displayName": "Jisun AN",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "xTonnmoY1a4Z",
    "outputId": "6eb45f5d-aacd-4825-c0c6-16cb9f0551cf"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re \n",
    "\n",
    "# Gensim for topic modeling\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim import matutils, models\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "import scipy.sparse\n",
    "\n",
    "# NLTK Stop words\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bw9U2l491a4Z"
   },
   "source": [
    "# 1. Getting the data\n",
    "\n",
    "We will use the COVID-19 Twitter dataset, which is collected based on COVID-19 related keywords, including covid, coronavirus, etc from Jan to April 2020. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "executionInfo": {
     "elapsed": 1097,
     "status": "ok",
     "timestamp": 1616416049455,
     "user": {
      "displayName": "Jisun AN",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "H3W6N99X1a4a",
    "outputId": "cddd6e0f-5a75-4783-ea31-ce16c4247fb2"
   },
   "outputs": [],
   "source": [
    "ori_df = pd.read_table(\"https://raw.githubusercontent.com/anjisun221/css_codes/main/sample_covid19_tweet_20200101_20200412_en.tsv\", sep=\"\\t\")\n",
    "\n",
    "print(ori_df.shape)\n",
    "ori_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 743,
     "status": "ok",
     "timestamp": 1616416051421,
     "user": {
      "displayName": "Jisun AN",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "AxjCDgDX1a4a",
    "outputId": "d2b37cc2-e344-450d-f28a-0d469e13a828"
   },
   "outputs": [],
   "source": [
    "df = ori_df.sample(n=5000, random_state=999)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 489,
     "status": "ok",
     "timestamp": 1616416051704,
     "user": {
      "displayName": "Jisun AN",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "-4Ex45vd1a4b",
    "outputId": "1fd39a60-dac6-42e9-8e0b-d1a145b177d6"
   },
   "outputs": [],
   "source": [
    "df.text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XxYnx_Ga1a4b"
   },
   "source": [
    "# (Preview) Let's build a quick LDA topic model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24714,
     "status": "ok",
     "timestamp": 1616415008087,
     "user": {
      "displayName": "Jisun AN",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "Bq_KfStv1a4b",
    "outputId": "d8284069-6467-4718-9b31-9324ce43dc3c"
   },
   "outputs": [],
   "source": [
    "# Convert to list\n",
    "data = df.text.values.tolist()\n",
    "data[:5]\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_words)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_words\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# Let's start with 2 topics.\n",
    "lda_model = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=10)\n",
    "lda_model.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 42558,
     "status": "ok",
     "timestamp": 1616415027202,
     "user": {
      "displayName": "Jisun AN",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "jQFcAN7d1a4c",
    "outputId": "5a2ec419-94bb-4c7b-9c37-c872649b9490"
   },
   "outputs": [],
   "source": [
    "# Let's start with 10 topics. This may take a while.\n",
    "lda_model = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=10, passes=10)\n",
    "lda_model.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O1DjRl151a4c"
   },
   "source": [
    "#### Does above topics make sense to you? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JzEZzYro1a4c"
   },
   "source": [
    "# 2. Data cleaning\n",
    "\n",
    "We will do the followings:\n",
    "* Remove @mention and url\n",
    "* Tokenization: We will use Gensim's module `gensim.utils.simple_preprocess` to tokenize the sentence in our corpus. It will convert a document into a list of tokens. Read more [here](https://tedboy.github.io/nlps/generated/generated/gensim.utils.simple_preprocess.html).\n",
    "* Removing Stop words\n",
    "* Bigram extraction- extracting list of two words frequently occurring together in the document e.g, ‘front_bumper’, ‘oil_leak’, ‘maryland_college_park’ etc.\n",
    "* Lemmatization\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 773,
     "status": "ok",
     "timestamp": 1616416054851,
     "user": {
      "displayName": "Jisun AN",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "NYhXQpkF1a4d",
    "outputId": "5038402b-6913-4afd-bd53-1600270a80be"
   },
   "outputs": [],
   "source": [
    "# Convert to list\n",
    "data = df.text.values.tolist()\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BIY8Mv5k1a4d"
   },
   "source": [
    "2-1. Remove @mention and url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 997,
     "status": "ok",
     "timestamp": 1616416055675,
     "user": {
      "displayName": "Jisun AN",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "EzjEtL_Y1a4d",
    "outputId": "ac8d9127-dfad-4e8a-80b8-5db9534fd821"
   },
   "outputs": [],
   "source": [
    "# Remove @mentions \n",
    "data = [re.sub(r'@\\w+', '', sent) for sent in data]\n",
    "\n",
    "# Remove urls (remove a word starting with http)\n",
    "data = [re.sub(r'http\\S+', '', sent) for sent in data]\n",
    "\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3r-NOVFQ1a4d"
   },
   "source": [
    "2-2. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 872,
     "status": "ok",
     "timestamp": 1616416057454,
     "user": {
      "displayName": "Jisun AN",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "xJcPJFEd1a4d",
    "outputId": "0512e491-b9f9-4aa6-a329-a75aae3528bb"
   },
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xUiumU5o1a4e"
   },
   "source": [
    "2-3. Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1039,
     "status": "ok",
     "timestamp": 1616416059438,
     "user": {
      "displayName": "Jisun AN",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "d206OL2_1a4e",
    "outputId": "d8e2d1f5-65d5-4ecb-b332-59c35d17b5b1"
   },
   "outputs": [],
   "source": [
    "# Prepare stopwords using NLTK\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# You can add other words to the list of stop words as well\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'amp'])\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "print(data_words_nostops[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UIbcK_xF1a4e"
   },
   "source": [
    "2-4. Bigram extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1613,
     "status": "ok",
     "timestamp": 1616416062996,
     "user": {
      "displayName": "Jisun AN",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "olcVzLcv1a4e",
    "outputId": "5e6bf950-0463-4257-f661-ea289725eb89"
   },
   "outputs": [],
   "source": [
    "# Build the bigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=10) # higher threshold fewer phrases.\n",
    "\n",
    "# Faster way to get a sentence clubbed as a bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "print(data_words_bigrams[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L-7jHc_A1a4e"
   },
   "source": [
    "2-5. Lemmatization\n",
    "\n",
    "Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma .\n",
    "\n",
    "Stemming and Lemmatization both generate the root form of the inflected words. The difference is that stem might not be an actual word whereas, lemma is an actual language word. Stemming follows an algorithm with steps to perform on the words which makes it faster.\n",
    "\n",
    "We use Spacy for lemmatization. \n",
    "It also allows to consider terms with a particular part of speech tag.\n",
    "We will use nouns (NOUN) and proper nouns (PNOUN) in this example. A proper noun is a specific name for a particular person, place, or thing. See options for part of speech here: https://spacy.io/usage/linguistic-features \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 41192,
     "status": "ok",
     "timestamp": 1616416801758,
     "user": {
      "displayName": "Jisun AN",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "F5d2NFJM1a4f",
    "outputId": "cc4dd03f-0062-4726-b585-9026ed839600"
   },
   "outputs": [],
   "source": [
    "def lemmatization(texts, allowed_postags=['NOUN']):\n",
    "    \"\"\"\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component\n",
    "# For normal use\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# For Colab use\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "print(\"Before Lemmatization:\", data_words_bigrams[:1])\n",
    "\n",
    "# Do lemmatization keeping only noun\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'PROPN'])\n",
    "\n",
    "print(\"After Lemmatization: \", data_lemmatized[:1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wHvuODww1a4f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0HMl2k4G1a4g"
   },
   "source": [
    "# 2. Building topic model\n",
    "\n",
    "The two main inputs to the LDA topic model are the dictionary (id2word) and the corpus. Let’s create them.\n",
    "\n",
    "Gensim creates a unique id for each word in the document (id2word). Then, the produced corpus is a mapping of (word_id, word_frequency).\n",
    "\n",
    "Check [`gensim.corpora`](https://radimrehurek.com/gensim/corpora/dictionary.html) for details about `filter_extremes()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 824,
     "status": "ok",
     "timestamp": 1616416849885,
     "user": {
      "displayName": "Jisun AN",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "OuUL25JP1a4g",
    "outputId": "299562c4-2a87-4f0b-92b8-0aac32db27c8"
   },
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "id2word.filter_extremes(no_below=1.5, no_above=0.8) # this will filter out words that are less frequen\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GLBsgQY41a4g"
   },
   "source": [
    "For example, (0, 1) above implies, word id 0 occurs once in the first document. Likewise, word id 1 occurs once and so on.\n",
    "\n",
    "This is used as the input by the LDA model.\n",
    "\n",
    "If you want to see what word a given id corresponds to, pass the id as a key to the dictionary (id2word)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 820,
     "status": "ok",
     "timestamp": 1616416851830,
     "user": {
      "displayName": "Jisun AN",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "lRcHp9431a4g",
    "outputId": "e7047ede-3866-4d76-9f4f-58face9ac9a7"
   },
   "outputs": [],
   "source": [
    "print(id2word[0], id2word[1], id2word[2], id2word[3], id2word[4])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mi84M-Uc1a4g"
   },
   "source": [
    "To build LDA model, you need to specify the number of topics apart from the dictionary (id2word) and the corpus. \n",
    "\n",
    "Passes is the total number of training passes. The larger passes would refine the assignment of words for topics. \n",
    "\n",
    "Check other parameters of LDA model [here](https://radimrehurek.com/gensim/models/ldamodel.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13944,
     "status": "ok",
     "timestamp": 1616416869737,
     "user": {
      "displayName": "Jisun AN",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "ouRtriRO1a4h",
    "outputId": "60e5cd53-7ed2-42db-9b82-b477d8b7fa00"
   },
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "\n",
    "# Let's start with 2 topics.\n",
    "lda_model = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=10)\n",
    "lda_model.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nE2uVECG1a4h"
   },
   "source": [
    "How to interpret a LDA topic?\n",
    "\n",
    "Topic 0 is a represented as  `0.050*\"coronavirus\" + 0.021*\"china\" + 0.011*\"case\" + 0.010*\"virus\" + 0.008*\"day\" + 0.007*\"cdc\" + 0.007*\"world\" + 0.007*\"death\" + 0.006*\"covid\" + 0.006*\"time\"`\n",
    "\n",
    "It means the top 10 keywords that contribute to this topic are: 'coronavirus', 'china', 'case', ... and so on and the weight of 'coronavirus' on topic 0 is 0.05.\n",
    "\n",
    "The weights reflect how important a keyword is to that topic. \n",
    "\n",
    "Looking at these keywords, can you guess what this topic could be? You may summarise it either are 'covid-19 update' or 'covid-19 news.'\n",
    "\n",
    "Likewise, Topic 1 could be 'Trump' or 'politics.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25264,
     "status": "ok",
     "timestamp": 1616416883281,
     "user": {
      "displayName": "Jisun AN",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "vyIoEjo91a4h",
    "outputId": "567dfb62-412e-4356-8ebc-aa3d72d36cdf",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 3 topics.\n",
    "lda_model = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=3, passes=10)\n",
    "lda_model.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35289,
     "status": "ok",
     "timestamp": 1616416893743,
     "user": {
      "displayName": "Jisun AN",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "awzsAU111a4h",
    "outputId": "955d0f42-1a40-4466-c34f-2f60911d52af",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 10 topics.\n",
    "lda_model = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=10, passes=10)\n",
    "lda_model.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gzLw_uWC1a4i"
   },
   "source": [
    "# 3. Topic coference \n",
    "\n",
    "Model perplexity and [topic coherence](https://rare-technologies.com/what-is-topic-coherence/) provide a convenient measure to judge how good a given topic model is. Topic Coherence measures score a single topic by measuring the degree of semantic similarity between high scoring words in the topic. There are two major types C_V typically 0 < x < 1 and uMass -14 < x < 14. When using c_v, the coference score of >0.5 would be considered to be good and it would be rare to see a > 0.9. See more [here](https://stackoverflow.com/questions/54762690/what-is-the-meaning-of-coherence-score-0-4-is-it-good-or-bad). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34076,
     "status": "ok",
     "timestamp": 1616416895455,
     "user": {
      "displayName": "Jisun AN",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "LwDoTCSo1a4i",
    "outputId": "c443da7a-e4e6-47bd-c5b2-b707af0e3e2b"
   },
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N-3_37vL1a4i"
   },
   "source": [
    "# 4. How to find the optimal number of topics for LDA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghNO-84J1a4i"
   },
   "source": [
    "To find the optimal number of topics, we will build many LDA models with different values of number of topics (k) and pick the one that gives the highest coherence value.\n",
    "\n",
    "Choosing a 'k' that marks the end of a rapid growth of topic coherence usually offers meaningful and interpretable topics. Picking an even higher value can sometimes provide more granular sub-topics.\n",
    "\n",
    "If you see the same keywords being repeated in multiple topics, it’s probably a sign that the ‘k’ is too large.\n",
    "\n",
    "We will use the elbow method, a visualization of changes of coherenve value by varying k, which gives us a graph of the optimal number of topics for greatest coherence. Check out [this](https://datascienceplus.com/evaluation-of-topic-modeling-topic-coherence/) for more for the elbow method.\n",
    "\n",
    "The compute_coherence_values() (see below) trains multiple LDA models and provides the models and their corresponding coherence scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wf6SbdDl1a4i"
   },
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        print(f'Training model for num_topics= {num_topics}')\n",
    "        model = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=num_topics, passes=10)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rkcAQyM51a4i"
   },
   "outputs": [],
   "source": [
    "start = 2\n",
    "limit = 60\n",
    "step = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 129033,
     "status": "ok",
     "timestamp": 1616417024502,
     "user": {
      "displayName": "Jisun AN",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "NgSQq1IM1a4j",
    "outputId": "1c23ff0d-6477-4b91-bbb5-293497d51d79"
   },
   "outputs": [],
   "source": [
    "# Can take a long time to run.\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_lemmatized, start=start, limit=limit, step=step)\n",
    "print('Completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "executionInfo": {
     "elapsed": 129036,
     "status": "ok",
     "timestamp": 1616417024513,
     "user": {
      "displayName": "Jisun AN",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "q1mhR9oQ1a4j",
    "outputId": "d0209143-21d6-4c9d-e97d-ce7f179de1dc"
   },
   "outputs": [],
   "source": [
    "# Show graph\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 129027,
     "status": "ok",
     "timestamp": 1616417024514,
     "user": {
      "displayName": "Jisun AN",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "ctC1Dq-j1a4j",
    "outputId": "58695edf-7bd2-4e82-cd72-7cbb54a6face"
   },
   "outputs": [],
   "source": [
    "# Print the coherence scores\n",
    "for m, cv in zip(x, coherence_values):\n",
    "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CddSV2uf1a4j"
   },
   "source": [
    "# 5. Best LDA model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "un_adUNp1a4j"
   },
   "source": [
    "So for further steps I will choose the model with 26 topics itself.\n",
    "By increasing the number of passes, the topics can be refined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6gFT2alU1a4j"
   },
   "outputs": [],
   "source": [
    "best_num_topics = 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 53746,
     "status": "ok",
     "timestamp": 1616417099954,
     "user": {
      "displayName": "Jisun AN",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "FrclY3Re1a4k",
    "outputId": "a0b4f41b-655c-44f7-9e58-b19dbafc81cb",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=best_num_topics, passes= 60)\n",
    "lda_model.print_topics()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5mJR--Mq1a4k"
   },
   "source": [
    "# 6. Visualize LDA topics\n",
    "\n",
    "pyLDAvis is designed to help users interpret the topics in a topic model that has been fit to a corpus of text data. The package extracts information from a fitted LDA topic model to inform an interactive web-based visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 861
    },
    "executionInfo": {
     "elapsed": 53861,
     "status": "ok",
     "timestamp": 1616417103187,
     "user": {
      "displayName": "Jisun AN",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "geW6Xtlj1a4k",
    "outputId": "ef7397e7-dd6a-4c86-f48b-a9e0b511ad55",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MbbCz6CV1a4k"
   },
   "source": [
    "So how to infer pyLDAvis’s output?\n",
    "\n",
    "Each bubble on the left-hand side plot represents a topic. The larger the bubble, the more prevalent is that topic.\n",
    "\n",
    "A good topic model will have fairly big, non-overlapping bubbles scattered throughout the chart instead of being clustered in one quadrant.\n",
    "\n",
    "A model with too many topics, will typically have many overlaps, small sized bubbles clustered in one region of the chart.\n",
    "\n",
    "Alright, if you move the cursor over one of the bubbles, the words and bars on the right-hand side will update. These words are the salient keywords that form the selected topic.\n",
    "\n",
    "Upnext, we will focus on how to arrive at the optimal number of topics given any large corpus of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zzj3N8oV1a4k"
   },
   "source": [
    "# 7. Finding the dominant topic in each sentence\n",
    "\n",
    "One of the practical application of topic modeling is to determine what topic a given document is about.\n",
    "\n",
    "To find that, we find the topic number that has the highest percentage contribution in that document.\n",
    "\n",
    "The `format_topics_sentences()` function below nicely aggregates this information in a presentable table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 66727,
     "status": "ok",
     "timestamp": 1616417119913,
     "user": {
      "displayName": "Jisun AN",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "G2NV5NLP1a4l",
    "outputId": "7b4557cf-4960-4cca-ff10-a78041159fbb"
   },
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel=lda_model, corpus=corpus):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['dominant_topic', 'topic_perc_contrib', 'keywords']\n",
    "    \n",
    "    return(sent_topics_df)\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus)\n",
    "df_topic_sents_keywords.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "executionInfo": {
     "elapsed": 1133,
     "status": "ok",
     "timestamp": 1616417121062,
     "user": {
      "displayName": "Jisun AN",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "ifFyXwSQ1a4l",
    "outputId": "33ac9a21-a166-422c-9fd3-21759f123c67"
   },
   "outputs": [],
   "source": [
    "# Combine the original data with inferred topics\n",
    "\n",
    "dominant_topic = pd.Series(df_topic_sents_keywords.dominant_topic.values.tolist())\n",
    "topic_perc_contrib = pd.Series(df_topic_sents_keywords.topic_perc_contrib.values.tolist())\n",
    "keywords = pd.Series(df_topic_sents_keywords.keywords.values.tolist())\n",
    "\n",
    "text_no = pd.Series(df.text_no.values.tolist())\n",
    "timestampStr = pd.Series(df.timestampStr.values.tolist())\n",
    "user_location_state = pd.Series(df.user_location_state.values.tolist())\n",
    "text = pd.Series(df.text.values.tolist())\n",
    "\n",
    "pd.set_option('display.max_colwidth', 150)\n",
    "new_df = pd.concat([text_no, timestampStr, user_location_state, dominant_topic, topic_perc_contrib, keywords, text], axis=1)\n",
    "new_df.columns = ['text_no', 'timestampStr', 'user_location_state', 'dominant_topic', 'topic_perc_contrib', 'keywords', 'text']\n",
    "new_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1121,
     "status": "ok",
     "timestamp": 1616417121062,
     "user": {
      "displayName": "Jisun AN",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "l_BRBnW61a4l",
    "outputId": "695eacf4-2b19-42e2-83ec-110a802cbf86"
   },
   "outputs": [],
   "source": [
    "new_df.dominant_topic.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1114,
     "status": "ok",
     "timestamp": 1616417121063,
     "user": {
      "displayName": "Jisun AN",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "Gh626FIE1a4m",
    "outputId": "184e67ca-dcbd-4171-ef96-f9dbc089e55a"
   },
   "outputs": [],
   "source": [
    "new_df.keywords.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "executionInfo": {
     "elapsed": 1611,
     "status": "ok",
     "timestamp": 1616417121568,
     "user": {
      "displayName": "Jisun AN",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "tL2jQXCw1a4m",
    "outputId": "59cc4e96-d2ab-4226-fda3-3548c57a2727"
   },
   "outputs": [],
   "source": [
    "plt.hist(new_df.dominant_topic, bins=best_num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "69fyjqHnB8gd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "neNQezElB-4U"
   },
   "source": [
    "# 8. Find the most representative documents for each topic. \n",
    "\n",
    "Sometimes just the topic keywords may not be enough to make sense of what a topic is about. So, to help with understanding the topic, you can find the documents a given topic has contributed to the most and infer the topic by reading that document. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 725
    },
    "executionInfo": {
     "elapsed": 752,
     "status": "ok",
     "timestamp": 1616417959078,
     "user": {
      "displayName": "Jisun AN",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "PkaC2VGi_usw",
    "outputId": "8af6cee6-5e5f-4692-85a8-1167acefb7aa"
   },
   "outputs": [],
   "source": [
    "# Group top 5 sentences under each topic\n",
    "sent_topics_sorteddf_lda = pd.DataFrame()\n",
    "\n",
    "sent_topics_outdf_grpd = new_df.groupby('dominant_topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_lda = pd.concat([sent_topics_sorteddf_lda, \n",
    "                                             grp.sort_values(['topic_perc_contrib'], ascending=[0]).head(3)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "sent_topics_sorteddf_lda.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# # Show\n",
    "sent_topics_sorteddf_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WUrEwhgZ_90E"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t4av606v1a4m"
   },
   "source": [
    "# Exercise 1\n",
    "\n",
    "Create a new topic model that includes terms from a different [part of speech](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) such as Adjectives and Verb and see if you can get better topics. \n",
    "\n",
    "After you complete and run below code, you will need to rerun almost all the above codes (from Section 2). \n",
    "See options for part of speech here: https://spacy.io/api/annotation\n",
    "\n",
    "Question to anwser: \n",
    "Find the best LDA model. How many topics does it have? \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WBmR46y51a4m"
   },
   "outputs": [],
   "source": [
    "print(\"Before Lemmatization:\", data_words_bigrams[:1])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = #WRITE YOUR CODE\n",
    "\n",
    "print(\"After Lemmatization: \", data_lemmatized[:1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b6RYnZ6u1a4n"
   },
   "source": [
    "# Exercise 2\n",
    "\n",
    "Chunksize controls how many documents are processed at a time in the LDA training algorithm. Increasing chunksize will speed up training, at least as long as the chunk of documents easily fit into memory. \n",
    "\n",
    "#### Exercise 2a) \n",
    "Update the `compute_coherence_values` function below (duplicated for you from Section 4) by setting the chunk size of the LdaModel.\n",
    "See [Set LdaModel parameters](https://radimrehurek.com/gensim/models/ldamodel.html).\n",
    "\n",
    "Rerun the compute_coherence_values function based on chunk sizes of 100 to 800 (both inclusive) in steps of 200. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yq9dTHku1a4n"
   },
   "outputs": [],
   "source": [
    "### Update the following compute_coherence_values function to define chunk size for LdaModel \n",
    "\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start, step):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various document chunksize\n",
    "\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    \n",
    "    for ??? in range(start, limit, step):\n",
    "        print(f'Training model based on chunk size= {???}')\n",
    "        model = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=best_num_topics, passes=60, ???)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QqJvp_Gn1a4n"
   },
   "outputs": [],
   "source": [
    "### Set values of chunk size and run this cell after updating compute_coherence_values function above \n",
    "\n",
    "# setting values for chunk size\n",
    "start = ??\n",
    "limit = ??\n",
    "step = ??\n",
    "\n",
    "# Run LdaModel\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_lemmatized, start=start, limit=limit, step=step)\n",
    "print('Completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6AqQK0I81a4n"
   },
   "source": [
    "#### Exercise 2b) \n",
    "Generate a coherence graph based on the chunk size defined and coherence values computed in Exercise 2a, where x-axis represents \n",
    "the 'Chunk Size' and y-axis represents 'Coherence score').\n",
    "\n",
    "Question to anwser: \n",
    "According to the graph, what is the most optimal chunk size? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ysl-godv1a4n"
   },
   "outputs": [],
   "source": [
    "## Write your code below\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ECPHD4qG1a4o"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "ncL7zRE_1a4T",
    "LLKFXxOW1a4Y",
    "bw9U2l491a4Z",
    "XxYnx_Ga1a4b",
    "O1DjRl151a4c"
   ],
   "name": "Lab7_topic_modeling_with_solution.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
