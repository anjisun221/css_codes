{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7rKcFrRb7uE"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/anjisun221/css_codes/blob/main/ay22t1/Lab03_text_analysis/Lab3_text_analysis_national_day_speech_ver2.1_Students.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pnxbJzbv_Jsr"
   },
   "source": [
    "# Lab 3 - Text-as-Data, National Day Speech analysis\n",
    "\n",
    "In this lab, you will learn:\n",
    "* How to clean texts\n",
    "* How to count words from texts and draw wordcloud from word frequencies\n",
    "* How to do comparative text analysis by extracting representative words from a corpus using lod-odds analysis\n",
    "\n",
    "This lab is written by Jisun AN (jisunan@smu.edu.sg) and Michelle KAN (michellekan@smu.edu.sg).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2976,
     "status": "ok",
     "timestamp": 1660975932018,
     "user": {
      "displayName": "Michelle Kan",
      "userId": "02256448236685590888"
     },
     "user_tz": -480
    },
    "id": "raj8qxaqb7uH",
    "outputId": "fbc86a09-0be2-4099-ede6-1691ae3606b3"
   },
   "outputs": [],
   "source": [
    "# Add Google Drive as an accessible path (Optional if you are running from Jupyter Notebook)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# change path to the designated google drive folder\n",
    "# otherwise, data will be saved in /content folder which you may have issue locating\n",
    "%cd /content/drive/My Drive/Colab Notebooks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 463,
     "status": "ok",
     "timestamp": 1660975937782,
     "user": {
      "displayName": "Michelle Kan",
      "userId": "02256448236685590888"
     },
     "user_tz": -480
    },
    "id": "T_VE-MBAb7uI"
   },
   "outputs": [],
   "source": [
    "mypath = \".\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cBqFOziVb7uI"
   },
   "source": [
    "# 0. import utility functions\n",
    "\n",
    "#### We've created a few utility functions. \n",
    "\n",
    "```read_word_count_file()``` : the input is csv file and the output is dictionary of word counts where the key is word and the value is the frequency of the words. e.g., {'singapore': 6472873} The input file must be a CSV file where each line is a pair of word and its counrt (e.g., singapore, 6472873).  \n",
    "\n",
    "\n",
    "```read_word_count_file_online()``` : this helps to import the data from online file. The input is url of the csv file and the output is dictionary of word counts where the key is word and the value is the frequency of the words. e.g., {'singapore': 6472873} The input file must be a CSV file where each line is a pair of word and its counrt (e.g., singapore, 6472873).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 791,
     "status": "ok",
     "timestamp": 1660975962107,
     "user": {
      "displayName": "Michelle Kan",
      "userId": "02256448236685590888"
     },
     "user_tz": -480
    },
    "id": "fD-JZumvb7uI",
    "outputId": "a97c6f94-9d21-414d-a6fb-570c0a7780d5"
   },
   "outputs": [],
   "source": [
    "### Let's download ```smt203util.py``` Below code should download the file in the same folder where your jupyter notebook is. \n",
    "!wget https://raw.githubusercontent.com/anjisun221/css_codes/main/ay22t1/Lab03_text_analysis/smt203util.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1660975962751,
     "user": {
      "displayName": "Michelle Kan",
      "userId": "02256448236685590888"
     },
     "user_tz": -480
    },
    "id": "ToKQR2VOb7uJ"
   },
   "outputs": [],
   "source": [
    "### Let's import all function from smt203util! \n",
    "from smt203util import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VBe-9KkA_Jsw"
   },
   "source": [
    "# 1. Getting the data\n",
    "\n",
    "In this lab, we will use Singapore's National Day speech data. \n",
    "\n",
    "Each line of the data is an extract of the National Day speech by the designated prime minister (ie. Lee Kuan Yew, Goh Chok Tong, Lee Hsien Loong)  of the specific year.  \n",
    "\n",
    "\"national_day_speech_1996_1991_2004_2020.tsv\" is a tab-separated file which fields are: \n",
    "- `speech` is a concatenated string of year and name of the prime minister e.g., 1966_lee_kuan_yew, 2004_lee_hsien_loong etc. who delivered the National Day speech\n",
    "- `line num` refers to the row number of the specific line of text in the National Day speech delivered by the prime minister of the specific year\n",
    "- `text` refers to the content of the specific line of text in the National Day speech\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1660975962752,
     "user": {
      "displayName": "Michelle Kan",
      "userId": "02256448236685590888"
     },
     "user_tz": -480
    },
    "id": "7Yig1Syu_Jsx"
   },
   "outputs": [],
   "source": [
    "### Import Pandas to analyze the data\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 877
    },
    "executionInfo": {
     "elapsed": 534,
     "status": "ok",
     "timestamp": 1660975963282,
     "user": {
      "displayName": "Michelle Kan",
      "userId": "02256448236685590888"
     },
     "user_tz": -480
    },
    "id": "6LXBIAIWb7uL",
    "outputId": "098f57e7-30f5-4ef1-ac84-f0e8615c5fd1"
   },
   "outputs": [],
   "source": [
    "### Read the file using Pandas 'read_table' function (either read_table, read_csv is fine)\n",
    "df = pd.read_table(\"https://raw.githubusercontent.com/anjisun221/css_codes/main/ay22t1/Lab03_text_analysis/national_day_speech_1996_1991_2004_2020.tsv\", sep=\"\\t\")\n",
    "# df = pd.read_table(\"./national_day_speech_1996_1991_2004_2020_extended.tsv\", sep=\"\\t\")\n",
    "\n",
    "### print the size of data frame (data frame == table)\n",
    "print(df.shape)\n",
    "df.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1660975963282,
     "user": {
      "displayName": "Michelle Kan",
      "userId": "02256448236685590888"
     },
     "user_tz": -480
    },
    "id": "11J35-a6b7uL",
    "outputId": "0dc8b138-a89f-4adb-bfa5-68b78223b967"
   },
   "outputs": [],
   "source": [
    "### to see the entire text \n",
    "pd.set_option('display.max_colwidth', 150)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1660975963283,
     "user": {
      "displayName": "Michelle Kan",
      "userId": "02256448236685590888"
     },
     "user_tz": -480
    },
    "id": "Uivr096_b7uM",
    "outputId": "6d2b7327-aaf6-4eac-a8da-480045de0210"
   },
   "outputs": [],
   "source": [
    "df.speech.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1660975963283,
     "user": {
      "displayName": "Michelle Kan",
      "userId": "02256448236685590888"
     },
     "user_tz": -480
    },
    "id": "t7geAGjEb7uM",
    "outputId": "c396776f-7451-4a18-cdaa-0fdf6bda8778"
   },
   "outputs": [],
   "source": [
    "df['speech'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1660975963903,
     "user": {
      "displayName": "Michelle Kan",
      "userId": "02256448236685590888"
     },
     "user_tz": -480
    },
    "id": "tVkvkwzJb7uM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "79d-nz2a_Js0"
   },
   "source": [
    "# 2. Cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "orQH1Z1P_Js0"
   },
   "source": [
    "When dealing with numerical data, data cleaning often involves removing null values and duplicate data, dealing with outliers, etc. With text data, there are some common data cleaning techniques, which are also known as text pre-processing techniques.\n",
    "\n",
    "With text data, this cleaning process can go on forever. There's always an exception to every cleaning step. So, we're going to follow the MVP (minimum viable product) approach - start simple and iterate. Here are a bunch of things you can do to clean your data. We're going to execute just the common cleaning steps here and the rest can be done at a later point to improve our results.\n",
    "\n",
    "**Common data cleaning steps on all text:**\n",
    "* Make text all lower case\n",
    "* Remove punctuation\n",
    "* Remove numerical values\n",
    "* Remove common non-sensical text (/n)\n",
    "* Tokenize text\n",
    "* Remove stop words\n",
    "\n",
    "**More data cleaning steps after tokenization:**\n",
    "* Stemming / lemmatization\n",
    "* Parts of speech tagging\n",
    "* Create bi-grams or tri-grams\n",
    "* Deal with typos\n",
    "* And more..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "egoiXlSq_Js0"
   },
   "source": [
    "## Round 1. Let's convert text to lowercase, remove punctuations, remove words containing numbers.\n",
    "\n",
    "Python has a built-in package called `re`, which can be used to work with **Regular Expressions**. Regular Expression, is a sequence of characters that forms a search pattern. It is widely used in text cleaning as it is very fast and useful approach for large volumes of text. <br>\n",
    "\n",
    "**Common expressions:**\n",
    " - \\d (lowercase d) matches digits, which means 0-9.\n",
    " - \\D (upper case D) matches any non-digits.\n",
    " - \\w (lowercase w) matches a \"word\" character: a letter or digit or underscore [a-zA-Z0-9_]\n",
    " - \\W (upper case W) matches any non-word character\n",
    " - \\s(lowercase s) matches a single whitespace character -- space, newline, return, tab, form [ \\n\\r\\t\\f]\n",
    " - \\S (upper case S) matches any non-whitespace character\n",
    " - \\[ ] contains a set of characters to match\n",
    " - '+' 1 or more occurrences of the pattern to its left, e.g. 'i+' = one or more i's\n",
    " - '*' 0 or more occurrences of the pattern to its left\n",
    " - '?' match 0 or 1 occurrences of the pattern to its left\n",
    "\n",
    "**Common 're' functions:**\n",
    "- re.sub(pattern,repl,text): replaces the pattern matches in text with repl. \n",
    "- re.findall(A, B): matches all instances of an expression A in a string B and returns them in a list.\n",
    "- re.search(A, B): matches the first instance of an expression A in a string B, and returns it as a re match object.\n",
    "- re.escape(A): automatically escape all metacharacters in string A\n",
    "e.g. `re.escape('abc')` returns `'\\a\\b\\c'`\n",
    "\n",
    "You can read up about `re` built-in functions [here](https://developers.google.com/edu/python/regular-expressions) and [here](https://docs.python.org/3/howto/regex.html#regex-howto).\n",
    "\n",
    "The following example uses the `re.sub()` function to remove punctuations and remove words containing number.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1660975973132,
     "user": {
      "displayName": "Michelle Kan",
      "userId": "02256448236685590888"
     },
     "user_tz": -480
    },
    "id": "JSKxJUWu_Js1"
   },
   "outputs": [],
   "source": [
    "# Apply a first round of text cleaning techniques\n",
    "import re \n",
    "import string\n",
    "\n",
    "def clean_text_round1(text):\n",
    "    '''Make text lowercase, remove punctuation and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.escape(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5KmpbDHNU2_h"
   },
   "source": [
    "Read up what `%s` means [here](https://www.geeksforgeeks.org/what-does-s-mean-in-a-python-format-string/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 372
    },
    "executionInfo": {
     "elapsed": 428,
     "status": "ok",
     "timestamp": 1660975979032,
     "user": {
      "displayName": "Michelle Kan",
      "userId": "02256448236685590888"
     },
     "user_tz": -480
    },
    "id": "SF4n0-7Z_Js1",
    "outputId": "81850a75-666f-4288-d147-9db174f63469"
   },
   "outputs": [],
   "source": [
    "# Let's take a look at the updated text\n",
    "df['clean_text_1'] = df['text'].apply(clean_text_round1)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dn6CSU54JzSG"
   },
   "source": [
    "## Additional exercise 1 (optional)\n",
    "\n",
    "1. Get more familar with regular expression below. \n",
    "\n",
    "2. Can you remove url from a tweet using regular expression? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 449,
     "status": "ok",
     "timestamp": 1660975982223,
     "user": {
      "displayName": "Michelle Kan",
      "userId": "02256448236685590888"
     },
     "user_tz": -480
    },
    "id": "OqzpLSMhJzSG",
    "outputId": "97612e29-d5c1-4719-ad62-9a12c0b9b8a9"
   },
   "outputs": [],
   "source": [
    "# Examples of Regular expression \n",
    "import re # Need module 're' for regular expression\n",
    "\n",
    "# Try find: re.findall(regexStr, inStr) -> matchedSubstringsList\n",
    "# r'...' denotes raw strings which ignore escape code, i.e., r'\\n' is '\\'+'n'\n",
    "# [0-9] matches any digit; [A-Za-z] matches any uppercase or lowercase letters.\n",
    "# + means one or more\n",
    "print(re.findall(r'[0-9]+', 'abc123xyz')) # Return a list of matched substrings.  \n",
    "print(re.findall(r'[0-9]+', 'abcxyz')) # Return []\n",
    "print(re.findall(r'[0-9]+', 'abc00123xyz456_0')) # Return ['00123', '456', '0']\n",
    "\n",
    "\n",
    "# Try substitute: re.sub(regexStr, replacementStr, inStr) -> outStr\n",
    "# Below code will replace all number block to *\n",
    "print(re.sub(r'[0-9]+', r'*', 'abc00123xyz456_0')) # Return 'abc*xyz*_*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "executionInfo": {
     "elapsed": 531,
     "status": "ok",
     "timestamp": 1660975986414,
     "user": {
      "displayName": "Michelle Kan",
      "userId": "02256448236685590888"
     },
     "user_tz": -480
    },
    "id": "OZLVtTHcJzSG",
    "outputId": "14011317-fb0a-44d5-d557-2b917127c61a"
   },
   "outputs": [],
   "source": [
    "# Here's an example tweet\n",
    "mytweet = \"New pre-print that @GruppiMauricio, @sibel_adali and I have been holding on to for a while: https://arxiv.org/abs/2101.10973. The goal was to leverage content sharing practices by news outlets in news veracity detection. Thread.\"\n",
    "mytweet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3vVAY8PqJzSH"
   },
   "source": [
    "Write the code to remove URL from the above tweet.\n",
    "\n",
    "As a result, you should see \n",
    "\n",
    "'New pre-print that @GruppiMauricio, @sibel_adali and I have been holding on to for a while:  The goal was to leverage content sharing practices by news outlets in news veracity detection. Thread.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2uqd85QNb7uP"
   },
   "outputs": [],
   "source": [
    "text = # Write your code \n",
    "text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BPvxSGDd_Js1"
   },
   "source": [
    "## Round 2. Let's remove stopwords. \n",
    "\n",
    "A stop word is a commonly used word (such as \"the\", \"a\", \"an\", \"in\"). For some analysis, like looking into top words, those stop words are often meaningless, and thus we remove them.\n",
    "\n",
    "The [Natural Language Toolkit (nltk)](https://www.nltk.org/api/nltk.html) is a Python package for natural language processing. We will import the library for the removal of stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 648,
     "status": "ok",
     "timestamp": 1660975991186,
     "user": {
      "displayName": "Michelle Kan",
      "userId": "02256448236685590888"
     },
     "user_tz": -480
    },
    "id": "0IG3CWjb_Js2",
    "outputId": "7b60b0b1-ea5d-4214-92fb-e0f82ab2a331"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UmiMDOChJzSI"
   },
   "source": [
    "The `nltk` library has a list of stopwords stored in 16 different languages. We will retrieve the list of English stop words using `stopwords.words('english')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1660975991635,
     "user": {
      "displayName": "Michelle Kan",
      "userId": "02256448236685590888"
     },
     "user_tz": -480
    },
    "id": "UhKTgIXoJzSI"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# Exclude stopwords with Python's list comprehension and pandas.DataFrame.apply.\n",
    "def clean_text_round2(text):\n",
    "    return ' '.join([word for word in text.split() if word not in (stop)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JWNTnZ2rJzSI"
   },
   "source": [
    "The expression `[word for word in x.split() if word not in (stop)]` is a ***list comprehension***.<br>\n",
    "List comprehension offers a shorter syntax when you want to create a new list based on the values of an existing list.<br> Syntax: `new_list = [expression for item in iterable list if condition == True]`<br>\n",
    "\n",
    "For example:<br>\n",
    "\n",
    "The following code snippet\n",
    "```python\n",
    "new_list = []\n",
    "for x in range(6):\n",
    "  if x % 2 == 0:\n",
    "      new_list.append(x * 2)\n",
    "```\n",
    "can be written in a more compact manner using list comprehension:\n",
    "```python\n",
    "new_list = [x * 2 for x in range(6) if x % 2 == 0]\n",
    "```\n",
    "\n",
    "**What does `[word for word in text.split() if word not in (stop)]` mean?**<br>\n",
    "<img align=\"center\" src='https://drive.google.com/uc?export=view&id=1QDEf1Hb_2MKQ9ULx7jWvAx0gxjh_aUxu'>\n",
    "\n",
    "You can read up about list comprehension [here](https://www.w3schools.com/python/python_lists_comprehension.asp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1660975995539,
     "user": {
      "displayName": "Michelle Kan",
      "userId": "02256448236685590888"
     },
     "user_tz": -480
    },
    "id": "gPXZ2o6k_Js2",
    "outputId": "a93e7a46-efd4-43d3-e2ad-b3cf24f4263d"
   },
   "outputs": [],
   "source": [
    "df['clean_text_2'] = df['clean_text_1'].apply(clean_text_round2)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p4HjENg8_Js2"
   },
   "source": [
    "**NOTE:** This data cleaning aka text pre-processing step could go on for a while, but we are going to stop for now. After going through some analysis techniques, if you see that the results don't make sense or could be improved, you can come back and make more edits such as:\n",
    "* Mark 'outstanding' and 'outstand' as the same word (stemming / lemmatization)\n",
    "* Combine 'thank you' into one term (bi-grams)\n",
    "* And a lot more..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VVQLUHQhb7uQ"
   },
   "source": [
    "# 3. Unigram analysis - Counting words (and save it to a file) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 481,
     "status": "ok",
     "timestamp": 1660975998277,
     "user": {
      "displayName": "Michelle Kan",
      "userId": "02256448236685590888"
     },
     "user_tz": -480
    },
    "id": "_BeL_osIb7uQ"
   },
   "outputs": [],
   "source": [
    "## Create ```word_counts``` folder \n",
    "import os\n",
    "os.makedirs('word_counts', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1660975998278,
     "user": {
      "displayName": "Michelle Kan",
      "userId": "02256448236685590888"
     },
     "user_tz": -480
    },
    "id": "bmxDpDlYb7uQ"
   },
   "outputs": [],
   "source": [
    "# this function create a dictionary with word counts from dataframe \n",
    "def count_words_from_dataframe(df):\n",
    "    result_dict = {}\n",
    "    # iterate rows of dataframe \n",
    "    for index, row in df.iterrows():\n",
    "        text = row['clean_text_2']\n",
    "        \n",
    "        # this will split a sentence into words \n",
    "        tokens = text.split()\n",
    "        \n",
    "        # iterate each word \n",
    "        for i in range(0, len(tokens)):\n",
    "            token = tokens[i]\n",
    "            try:\n",
    "                result_dict[token] += 1\n",
    "            except KeyError:\n",
    "                result_dict[token] = 1\n",
    "                    \n",
    "    return result_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 550,
     "status": "ok",
     "timestamp": 1660976002560,
     "user": {
      "displayName": "Michelle Kan",
      "userId": "02256448236685590888"
     },
     "user_tz": -480
    },
    "id": "8sC1182Sb7uQ",
    "outputId": "d7303e6a-fa7a-4c55-b5ba-9b07af4e3338"
   },
   "outputs": [],
   "source": [
    "list_of_speeches = list(df.speech.unique())\n",
    "list_of_speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2214,
     "status": "ok",
     "timestamp": 1660976004773,
     "user": {
      "displayName": "Michelle Kan",
      "userId": "02256448236685590888"
     },
     "user_tz": -480
    },
    "id": "Y3XWlSnqb7uQ",
    "outputId": "be39095e-338e-4163-ea8f-90bc86d39f9f"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "count_type = 'unigram'\n",
    "\n",
    "for each_speech in list_of_speeches:\n",
    "    print(each_speech)\n",
    "    \n",
    "    ## query helps to filter rows of dataframe given a condition \n",
    "    df_speech = df.query('speech == @each_speech')\n",
    "    print(df_speech.shape)\n",
    "    \n",
    "    ## this function will return a dictionary of words and frequency\n",
    "    result = count_words_from_dataframe(df_speech)\n",
    "\n",
    "    ## sorting the words based on their frequency\n",
    "    sorted_dic = ((k, result[k]) for k in sorted(result, key=result.get, reverse=True))\n",
    "    \n",
    "    ## write the dictionary in a file\n",
    "    with open(f\"./word_counts/{each_speech}_{count_type}_counts.csv\", 'w') as fp:\n",
    "        writer = csv.writer(fp, delimiter=',')\n",
    "        writer.writerows(sorted_dic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3jKPb2cSb7uR"
   },
   "source": [
    "## Let's draw WordCloud using unigrams!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 655,
     "status": "ok",
     "timestamp": 1660976005419,
     "user": {
      "displayName": "Michelle Kan",
      "userId": "02256448236685590888"
     },
     "user_tz": -480
    },
    "id": "Jy-auBBJb7uR",
    "outputId": "fdc7ccbd-def1-42a1-9828-95dfd79daa52"
   },
   "outputs": [],
   "source": [
    "## This it OPTIONAL if you are running the current notebook using Google Colab\n",
    "!conda install --yes -c conda-forge wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1660976007884,
     "user": {
      "displayName": "Michelle Kan",
      "userId": "02256448236685590888"
     },
     "user_tz": -480
    },
    "id": "kZ5dGMY-b7uR"
   },
   "outputs": [],
   "source": [
    "### Import relevant libraries\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "from PIL import Image\n",
    "from os import path\n",
    "import csv\n",
    "\n",
    "### this function will creat wordcloud based on word frequencies and save them into files under plot. \n",
    "def makeImage(termDict, outputfile):\n",
    "    \n",
    "    wc = WordCloud(max_font_size=60, width=1280, height=720, background_color=\"white\")    \n",
    "\n",
    "    ### generate word cloud using frequencies!\n",
    "    wc.generate_from_frequencies(termDict)\n",
    "    wc.to_file(outputfile+\".png\")\n",
    "    wc.to_file(outputfile+\".pdf\")\n",
    "    \n",
    "    ### show the figure\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1660976011080,
     "user": {
      "displayName": "Michelle Kan",
      "userId": "02256448236685590888"
     },
     "user_tz": -480
    },
    "id": "3DbPvL9Sb7uR"
   },
   "outputs": [],
   "source": [
    "### Create ```plot``` folder \n",
    "os.makedirs('plot', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "executionInfo": {
     "elapsed": 3351,
     "status": "ok",
     "timestamp": 1660976015066,
     "user": {
      "displayName": "Michelle Kan",
      "userId": "02256448236685590888"
     },
     "user_tz": -480
    },
    "id": "ziA4Rfljb7uR",
    "outputId": "28e601a1-4efb-4a63-f39a-0d56cec37c43"
   },
   "outputs": [],
   "source": [
    "speech_name = \"2004_lee_hsien_loong\"\n",
    "### We're using one of the utility function! \"read_word_count_file\"\n",
    "fullTermsDict = read_word_count_file(f'{mypath}/word_counts/{speech_name}_unigram_counts.csv')\n",
    "outputfile = f\"./plot/wordcloud_count_unigram_{speech_name}\"\n",
    "makeImage(fullTermsDict, outputfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "executionInfo": {
     "elapsed": 3531,
     "status": "ok",
     "timestamp": 1660976018594,
     "user": {
      "displayName": "Michelle Kan",
      "userId": "02256448236685590888"
     },
     "user_tz": -480
    },
    "id": "Qlkd9Ikeb7uR",
    "outputId": "7d19f4fc-aa73-4b98-db34-47c04e735441",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "speech_name = \"2020_lee_hsien_loong\"\n",
    "### We're using one of the utility function! \"read_word_count_file\"\n",
    "fullTermsDict = read_word_count_file(f'{mypath}/word_counts/{speech_name}_unigram_counts.csv')\n",
    "outputfile = f\"./plot/wordcloud_count_unigram_{speech_name}\"\n",
    "makeImage(fullTermsDict, outputfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lT57hRIAb7uR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G3mOmzjeb7uS"
   },
   "source": [
    "### Exercise 1. Count bigram \n",
    "\n",
    "You task is to write a function named ```count_bigrams_from_dataframe``` whose input is dataframe, ```df``` and output is the dictionary of bigrams with their counts. \n",
    "\n",
    "Then, can you also save them into file after sorting the bigrams based on their counts? \n",
    "\n",
    "(Optional) You may draw word clouds with bigrams as well! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 602,
     "status": "ok",
     "timestamp": 1660974838208,
     "user": {
      "displayName": "Michelle Kan",
      "userId": "02256448236685590888"
     },
     "user_tz": -480
    },
    "id": "rd4_AseAb7uS"
   },
   "outputs": [],
   "source": [
    "def count_bigrams_from_dataframe(df):\n",
    "    \n",
    "    result_dict = {}                    \n",
    "    for index, row in df_speech.iterrows():\n",
    "        text = row['clean_text_2']\n",
    "        tokens = text.split()\n",
    "        \n",
    "        ## write your code \n",
    "        \n",
    "    return result_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1660974838765,
     "user": {
      "displayName": "Michelle Kan",
      "userId": "02256448236685590888"
     },
     "user_tz": -480
    },
    "id": "2RKLs4pWb7uS",
    "outputId": "df9d6f70-6865-4713-f492-3e80dc39f854"
   },
   "outputs": [],
   "source": [
    "count_type = 'bigram'\n",
    "\n",
    "for each_speech in list_of_speeches:\n",
    "    print(each_speech)\n",
    "    df_speech = df.query('speech == @each_speech')\n",
    "    print(df_speech.shape)\n",
    "    \n",
    "    ## write your code \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (optional) draw word clouds with bigrams \n",
    "\n",
    "## write your code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5BMMxMoRb7uS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lCPz74WQb7uS"
   },
   "source": [
    "# 4. Find representative words by using log odds \n",
    "\n",
    "### In this example, we will compare 2004 and 2020 National Day speeches by PM Lee Hsien Loong. \n",
    "\n",
    "We will detect unigrams that are over-represented in either 2004 or 2020 speech. \n",
    "\n",
    "To do so, we use log-odds ratios with informative Dirichlet priors (Monroe, Colaresi, and Quinn 2008). This method estimates the log-odds ratio of each word between two corpora i and j given the prior frequencies obtained from a back ground corpus. \n",
    "\n",
    "This method identifies representative words of a corpus i in comparison with another corpus j and the common corpus (global/background corpus). \n",
    "\n",
    "#### We've created a few functions for computing log odds values and finding the representativ words.\n",
    "\n",
    "```calculate_log_odds_idp(global_counts, counts_i_name, counts_i_dict, counts_j_name, counts_j_dict)``` : it returns the log odds value (```log_odds_z_score```) for each word, which indicates the significance of the word in the corresponding corpus. You should provide background corpus (we call ```global_counts```), two names of the corpora: ```counts_i_name``` and ```counts_j_name``` (for naming the file), and two dictionaries of word counts from each corpus: ```counts_i_dict``` and ```counts_j_dict```. \n",
    "\n",
    "```find_discriminative_words(top_words_df, threshold_i, threshold_j, num_i, num_j, mypath)```: This function will filter out those words that appear less than k times in each corpus (`threshold_i` and ```threshold_j```), rank the words based on lod-odds z-scores and select top-N representative words (```num_i```, ```num_j```), and generate a csv file where each row contains (word, log_odds_z_score) for each of the two corpora .  \n",
    "\n",
    "For details about how to compute log odds z-score, see: \"Monroe, B. L.; Colaresi, M. P.; and Quinn, K. M. 2008. Fightin’words: Lexical feature selection and evaluation for identifying the content of political conflict. Political Analysis 16(4):372–403.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3426,
     "status": "ok",
     "timestamp": 1660976026079,
     "user": {
      "displayName": "Michelle Kan",
      "userId": "02256448236685590888"
     },
     "user_tz": -480
    },
    "id": "jHsf1g3Ib7uS",
    "outputId": "fc477cdd-7298-4a1e-cd77-aef05f1651f3"
   },
   "outputs": [],
   "source": [
    "### Getting global word counts (frequency of unigram in Google News datasets)\n",
    "target_url = \"https://raw.githubusercontent.com/anjisun221/css_codes/main/ay22t1/Lab03_text_analysis/1gram_englishall_count.csv\"\n",
    "global_counts = read_word_count_file_online(target_url)\n",
    "print(\"The number of unigrams=\", len(global_counts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10055,
     "status": "ok",
     "timestamp": 1660976038956,
     "user": {
      "displayName": "Michelle Kan",
      "userId": "02256448236685590888"
     },
     "user_tz": -480
    },
    "id": "dE-1axzYb7uT",
    "outputId": "826654f9-3d76-4028-89d2-195612ea503c"
   },
   "outputs": [],
   "source": [
    "### Removing stopwords from the word list and \n",
    "stopwords_list = get_stopwords()\n",
    "global_counts = {k: v for k, v in global_counts.items() if (k not in stopwords_list) and (len(k) > 2)}\n",
    "print(\"After excluding stop words: \", len(global_counts))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lQ3bEPoub7uT"
   },
   "source": [
    "#### Let's load word frequency file and create dictionary of word counts for the two speeches!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1660976038956,
     "user": {
      "displayName": "Michelle Kan",
      "userId": "02256448236685590888"
     },
     "user_tz": -480
    },
    "id": "NKnAPwRab7uT"
   },
   "outputs": [],
   "source": [
    "# counts_i_name = \"1966_lee_kuan_yew\"\n",
    "# counts_i_name = \"1991_goh_chok_tong\"\n",
    "counts_i_name = \"2004_lee_hsien_loong\"\n",
    "# counts_j_name = \"2020_lee_hsien_loong\"\n",
    "counts_i = read_word_count_file(f\"{mypath}/word_counts/{counts_i_name}_unigram_counts.csv\")\n",
    "\n",
    "### we filter out words that exist in our background corpus\n",
    "counts_i_dict = {k: v for k, v in counts_i.items() if k in global_counts}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1660976038956,
     "user": {
      "displayName": "Michelle Kan",
      "userId": "02256448236685590888"
     },
     "user_tz": -480
    },
    "id": "bcogtInib7uT"
   },
   "outputs": [],
   "source": [
    "# counts_j_name = \"1966_lee_kuan_yew\"\n",
    "# counts_j_name = \"1991_goh_chok_tong\"\n",
    "# counts_j_name = \"2004_lee_hsien_loong\"\n",
    "counts_j_name = \"2020_lee_hsien_loong\"\n",
    "counts_j = read_word_count_file(f\"{mypath}/word_counts/{counts_j_name}_unigram_counts.csv\")\n",
    "\n",
    "### we filter out words that exist in our background corpus\n",
    "counts_j_dict = {k: v for k, v in counts_j.items() if k in global_counts}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3930,
     "status": "ok",
     "timestamp": 1660976043566,
     "user": {
      "displayName": "Michelle Kan",
      "userId": "02256448236685590888"
     },
     "user_tz": -480
    },
    "id": "lF8Cmj8Cb7uT"
   },
   "outputs": [],
   "source": [
    "### this function will return log-odds values. \n",
    "top_words_df = calculate_log_odds_idp(global_counts, counts_i_name, counts_i_dict, counts_j_name, counts_j_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GjXszenkb7uT"
   },
   "source": [
    "### Interpretation lad z-score (log_odds_z_score) \n",
    "\n",
    "Consider it as a value that indicate the significance of the word in a corpus. Given that we have two corpora, i and j, the order matters when interpreting this z-score. For corpus i, the lower the z-score, more significant the word is while for corpus j, the higher the z-score, more significant the word is.\n",
    "\n",
    "The word frequency is also important, if the word frequency is low, those words may not be meaningful. So, we filter words based on the two condition: word frequency and z-score. \n",
    "\n",
    "In the below example, \n",
    "for corpus i, we will consider words that have lower z-scores and that appear at least twice in the corpus. \n",
    "\n",
    "For corpus j, we will consider words that have higher z-scores and that appear at least twice in the corpus. \n",
    "\n",
    "Note that, we choose 2 as a threshold for word frequency, but usually it should be much higher if we have larger corpus. Our speech data is small, so we have to choose 2. Normally, we would use 50 or 100 as a minimum number of word frequency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 676
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1660976043567,
     "user": {
      "displayName": "Michelle Kan",
      "userId": "02256448236685590888"
     },
     "user_tz": -480
    },
    "id": "ZHvuc-lGb7uT",
    "outputId": "9d18f3a1-c1c2-496e-8329-7d30a2e4a3ed"
   },
   "outputs": [],
   "source": [
    "### Below shows the representative words of 2004 National Day speech by PM Lee. \n",
    "top_words_df[top_words_df[counts_i_name] >= 2].head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 676
    },
    "executionInfo": {
     "elapsed": 429,
     "status": "ok",
     "timestamp": 1660976046554,
     "user": {
      "displayName": "Michelle Kan",
      "userId": "02256448236685590888"
     },
     "user_tz": -480
    },
    "id": "3-EIpI61b7uT",
    "outputId": "74477a6a-0ebd-447e-aa37-f8c1873e3e36"
   },
   "outputs": [],
   "source": [
    "### Below shows the representative words of 2020 National Day speech by PM Lee. \n",
    "top_words_df[top_words_df[counts_j_name] >= 2].iloc[::-1].head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1065,
     "status": "ok",
     "timestamp": 1660976049853,
     "user": {
      "displayName": "Michelle Kan",
      "userId": "02256448236685590888"
     },
     "user_tz": -480
    },
    "id": "aXzKuALyb7uT"
   },
   "outputs": [],
   "source": [
    "### Below function will simply help you to print the above table into file, so that we can use it for drawing word cloud. \n",
    "### The output of this function is a csv file where each row contains (word, log_odds_z_score) for the two corpora. \n",
    "### threshold_i and threshold_j are the threshold of word frequency. \n",
    "### num_i and num_j are the number of representative words it will write in file. \n",
    "find_discriminative_words(top_words_df, threshold_i=2, threshold_j=2, num_i=20, num_j=20, mypath='.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOvpUfCNb7uT"
   },
   "source": [
    "Note that the above function (`find_discriminative_words()`) results in creating two files where each row is a word and its log-odds value and each file is for each speech!!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGplok3qb7uU"
   },
   "source": [
    "## Let's draw WordClouds based on log odds values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4l5y51pb7uU"
   },
   "source": [
    "### Exercise 2. Draw WordClouds based on the log odds values for each speech and write the WordClouds in a file. \n",
    "\n",
    "You can use the resulting files of `find_discriminative_words()` to draw the wordcloud. \n",
    "\n",
    "Hint 1: You can simply consider \"log-odds value\" as a frequency. \n",
    "Hint 2: Note that you need to create one wordcloud for each speech. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H2o8JqtYb7uU"
   },
   "outputs": [],
   "source": [
    "### Draw a WordCloud for 2004 National Day speech by PM Lee.\n",
    "\n",
    "[Write your code] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "479BXsjbb7uU"
   },
   "outputs": [],
   "source": [
    "# Draw a WordCloud for 2020 National Day speech by PM Lee. \n",
    "\n",
    "[Write your code] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CHHUS9Fsb7uU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab3_text_analysis_national_day_speech - Solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
